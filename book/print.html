<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Operating Systems: Book Summary</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="chapter_1.html"><strong aria-hidden="true">1.</strong> The Process</a></li><li class="chapter-item expanded "><a href="chapter_2.html"><strong aria-hidden="true">2.</strong> Process API</a></li><li class="chapter-item expanded "><a href="chapter_3.html"><strong aria-hidden="true">3.</strong> Mechanism - Limited Direct Execution</a></li><li class="chapter-item expanded "><a href="chapter_4.html"><strong aria-hidden="true">4.</strong> Scheduling</a></li><li class="chapter-item expanded "><a href="chapter_5.html"><strong aria-hidden="true">5.</strong> Multi-Level Feedback Queue</a></li><li class="chapter-item expanded "><a href="chapter_6.html"><strong aria-hidden="true">6.</strong> Proportional Share</a></li><li class="chapter-item expanded "><a href="chapter_7.html"><strong aria-hidden="true">7.</strong> Address Spaces</a></li><li class="chapter-item expanded "><a href="chapter_8.html"><strong aria-hidden="true">8.</strong> Mechanism - Address Translation</a></li><li class="chapter-item expanded "><a href="chapter_9.html"><strong aria-hidden="true">9.</strong> Segmentation</a></li><li class="chapter-item expanded "><a href="chapter_10.html"><strong aria-hidden="true">10.</strong> Free-Space Management</a></li><li class="chapter-item expanded "><a href="chapter_11.html"><strong aria-hidden="true">11.</strong> Paging - Introduction</a></li><li class="chapter-item expanded "><a href="chapter_12.html"><strong aria-hidden="true">12.</strong> Paging - Faster Translation</a></li><li class="chapter-item expanded "><a href="chapter_13.html"><strong aria-hidden="true">13.</strong> Paging - Smaller Tables</a></li><li class="chapter-item expanded "><a href="chapter_14.html"><strong aria-hidden="true">14.</strong> Beyond Physical Memory - Mechanism</a></li><li class="chapter-item expanded "><a href="chapter_15.html"><strong aria-hidden="true">15.</strong> Physical Memory - Policies</a></li><li class="chapter-item expanded "><a href="chapter_17.html"><strong aria-hidden="true">16.</strong> Concurrency - An Introduction</a></li><li class="chapter-item expanded "><a href="chapter_18.html"><strong aria-hidden="true">17.</strong> Thread API</a></li><li class="chapter-item expanded "><a href="chapter_19.html"><strong aria-hidden="true">18.</strong> Locks</a></li><li class="chapter-item expanded "><a href="chapter_20.html"><strong aria-hidden="true">19.</strong> Lock-Based Concurrent Data Structures</a></li><li class="chapter-item expanded "><a href="chapter_21.html"><strong aria-hidden="true">20.</strong> Condition Variables</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Operating Systems: Book Summary</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="1-the-process"><a class="header" href="#1-the-process">1. The process</a></h1>
<p><em>Definition: It is a running program</em></p>
<h2 id="crux-of-the-problem-how-to-provide-the-illusion-of-many-cpus"><a class="header" href="#crux-of-the-problem-how-to-provide-the-illusion-of-many-cpus">Crux of the problem: How to provide the illusion of many CPUs?</a></h2>
<hr />
<p>OS creates illusion of many CPUs by virtualizing the CPU.</p>
<ul>
<li>Running one process, stopping it and running another, and so forth -&gt; Promotes the illusion that many virtual CPUs exists when in fact there is only one. </li>
<li>To implement this, the OS needs: 
<ul>
<li>Mechanisms (Low level machinery): Low level methods or protocols that implement a needed piece of functionality.</li>
<li>Policies (intelligence): Algorithms for making some kind of decisions within the OS (i.e. the scheduler is an algorithms that makes the decision of which process to run next, don't worry if you don't know yet what an scheduler is, will get to that later).</li>
</ul>
</li>
</ul>
<p><em>Formal definition: Abstraction provided by the OS of a running program</em> </p>
<p>At any given type, we can summarize a process by taking an inventory of the different pieces of the system it accesses or affects during the course of its execution</p>
<h2 id="components-of-a-process"><a class="header" href="#components-of-a-process">Components of a process:</a></h2>
<ol>
<li>Memory (a.k.a address space): Instructions, data that the programs read and writes sits in memory.</li>
<li>Registers; Many instruction read or update registers.</li>
<li>Program counter (a.k.a instruction pointer): Tell us which instruction of the program will execute next.</li>
<li>Persistent storage devices: I/O information might include list of files the process currently has open. </li>
</ol>
<h2 id="process-api"><a class="header" href="#process-api">Process API</a></h2>
<p>Usually a process interface of an operating systems includes the following:</p>
<ul>
<li>Create: Some method to create a new process. </li>
<li>Destroy: Interface to destroy processes forcefully.</li>
<li>Wait: Waiting interface, to wait for a process to stop running.</li>
<li>Miscellaneous control: Other control that are possible (i.e methods to suspend process, and then resume it).</li>
<li>Status: Interface to get status information of process. </li>
</ul>
<h2 id="process-creation"><a class="header" href="#process-creation">Process creation</a></h2>
<ol>
<li>OS reads executable bytes of executable file from disks and place them in memory somewhere.
<em>On simpler OS's the loading process is done eagerly (all at once), in most advanced OS's, this is done lazily (i.e by loading pieces of code or data only as they are needed during program execution)</em></li>
<li>OS allocates memory for the program's run-time stack, and it initializes the stack with arguments. </li>
<li>OS may also allocate some memory for the program's heap. The OS may get involved and allocate more memory to the process to help satisfy heap memory calls (i.e <code>malloc</code> on C)</li>
<li>OS makes I/O initialization tasks. i.e on Unix like OS, file decriptors 0, 1 and 2 gets assigned to sderr, stdio, stdin. </li>
<li>Last task for the OS: Start the program running at the entry point, namely <code>main()</code>, transferring control of the CPU to the newly-created process. </li>
</ol>
<h2 id="process-states"><a class="header" href="#process-states">Process states</a></h2>
<p>In a simplified view, a process can be in one of three states:</p>
<ul>
<li>Running: Process is executing instructions. </li>
<li>Ready: The process is ready to run, but the OS has chosen not to start it.</li>
<li>Blocked: Process has performed some kind of operation that makes it not ready to run until some other event takes place. </li>
</ul>
<h2 id="data-structures"><a class="header" href="#data-structures">Data structures</a></h2>
<p>To keep track of the state of each process, the OS keeps some kind of <strong>process list</strong> for all processes that are ready and some additional information to track which process is currently running. </p>
<center><img src="./images/ProcessStruct.png"></center>
<p>In the image we can see the different states a process can be, and what information does the <code>xv6</code> kernel keeps about a process.</p>
<div style="break-before: page; page-break-before: always;"></div><p>todo</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="3-mechanism---limited-direct-execution"><a class="header" href="#3-mechanism---limited-direct-execution">3. Mechanism - Limited Direct Execution</a></h1>
<p><em>CPU virtualization: Run one process for a little while, then another one, and so forth. By time sharing the CPU, virtualization is achieved.</em></p>
<ul>
<li>Challenges in building this virtualization: 
<ol>
<li>Performance: How to implement without adding excessive overhead to the system?</li>
<li>Control: How to run processes efficiently while retaining control over the CPU?</li>
</ol>
</li>
</ul>
<h2 id="crux-how-to-efficiently-virtualize-the-cpu-with-control"><a class="header" href="#crux-how-to-efficiently-virtualize-the-cpu-with-control">Crux: How to efficiently virtualize the CPU with control?</a></h2>
<h2 id="basic-technique-limited-direct-execution"><a class="header" href="#basic-technique-limited-direct-execution">Basic technique: Limited direct execution</a></h2>
<ul>
<li>&quot;Direct Execution&quot; part: Run the program directly on the CPU
<ul>
<li>OS wishes to start a program running: 
<ol>
<li>OS creates a process entry for it in a process list.</li>
<li>Allocate some memory for it.</li>
<li>Loads the program code into memory (from disk)</li>
<li>Locates its entry point, jumps to it and starts running the users' code</li>
</ol>
<ul>
<li>Problems:
<ol>
<li>How can the OS make sure the program doesn't do anything that we don't want it to do, while still running efficiently.</li>
<li>How to do the context switch (stop the program and switch to another process) to time share the CPU thus virtualizing it. </li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="problem-1-restricted-operations"><a class="header" href="#problem-1-restricted-operations">Problem #1: Restricted operations</a></h2>
<p><strong>Crux: A process must be able to perform I/O and some other restricted operations, but without giving the process complete control over the system. How can the OS and hardware work together to do so?</strong></p>
<ul>
<li>Approach: Introduce a new processor mode: <em>user mode</em> 
<ul>
<li>Code that runs in user mode is restricted in what it can do (i.e it can't issue I/O requests)</li>
<li>Contrast to <em>user mode</em>: <em>kernel mode</em> - Code that runs in <em>kernel mode</em> can do what it likes.</li>
</ul>
</li>
</ul>
<p>How can we perform some kind of privileged operation (such as reading from disk) if we are on user mode?</p>
<ul>
<li>We provide the user the ability to do <strong>system calls</strong>. </li>
</ul>
<h3 id="system-calls"><a class="header" href="#system-calls">System calls</a></h3>
<p>They allow the kernel to carefully expose certain key pieces of functionality to user programs, such as:</p>
<ul>
<li>Accessing the file system</li>
<li>Creating and destroying processes</li>
<li>Communicating with other processes </li>
<li>Allocating more memory
Most OS provide a few hundred calls.</li>
</ul>
<h4 id="how-to-do-a-system-call"><a class="header" href="#how-to-do-a-system-call">How to do a system call?</a></h4>
<ul>
<li>Program executes a special <strong>trap</strong> instruction. </li>
<li>Trap instruction jumps into the kernel and raises the privilege to kernel  mode</li>
<li>Once in kernel mode, the system can now perform whatever privileged operation that's needed (if allowed), hence doing the required work  for the calling process. </li>
<li>When it operation it's finished, OS calls a special <strong>return-from-trap</strong> instruction that returning into the calling user program while reducing the privilege level back to user mode. </li>
</ul>
<h4 id="how-does-the-trap-knows-which-code-to-run-inside-the-os"><a class="header" href="#how-does-the-trap-knows-which-code-to-run-inside-the-os">How does the trap knows which code to run inside the OS?</a></h4>
<ul>
<li>Clearly the calling process can't specify an address to jump to (bad idea since you will be jumping to anywhere into the kernel)</li>
<li>Kernel controls what code executes upon a trap by setting up a trap table at boot time.</li>
<li>The OS tells the hardware what code to run when a certain exception event occurs. </li>
<li>To specify an exact system call, a system-call number is usually assigned to each system call. </li>
<li>The user code, is responsible for placing the desired system-call number in a register or location on the stack. </li>
<li>The OS, when handling the system call inside the trap handle, examines the number, ensures it's valid and executes the corresponding code if it is.</li>
</ul>
<h3 id="example"><a class="header" href="#example">Example:</a></h3>
<p>Process want to read a file. </p>
<ul>
<li>Program calls the <code>read</code> function with the proper function arguments. </li>
<li><code>read</code> loads the system call number <code>5</code> (for xv6-riscv) into the <code>a7</code> register</li>
<li><code>read</code> execute the <code>ecall</code> instruction to trigger the exception (trap)</li>
<li>Now on kernel mode, it sees that an exception/trap has occurred</li>
<li>The kernel saves the user registers on to a special place on memory/cpu</li>
<li>The kernel reads the <code>a7</code> register, saves the number and calls the system call with the corresponding number</li>
<li>When the system call finishes, it places the resulting return value into register <code>a0</code> of the user program</li>
<li>Kernel calls <code>usertrapret</code> function which returns to user space and restores the cpu registers</li>
<li>Now back to user space, the user program continues the execution with the result of the system call. </li>
</ul>
<h2 id="problem-2-switching-between-processes"><a class="header" href="#problem-2-switching-between-processes">Problem #2: Switching between processes</a></h2>
<p><strong>Crux: How can the operating system regain control of the CPU so that it can switch between processes?</strong></p>
<h3 id="cooperative-approach"><a class="header" href="#cooperative-approach">Cooperative approach</a></h3>
<p>The OS <em>trusts</em> the processes of the system to behave reasonably. Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task. </p>
<ul>
<li>Most processes transfer control of the CPU to the OS by making system calls.</li>
<li>Processes transfer control to the OS when they do something illegal (dividing by zero, trying to access memory that it shouldn't, etc)
Problems: </li>
<li>Process ends up on an infinite loop, the OS never regains control</li>
<li>Malicious process intentionally never gives up the CPU</li>
</ul>
<h3 id="non-cooperative-approach-the-os-take-control"><a class="header" href="#non-cooperative-approach-the-os-take-control">Non-Cooperative Approach: The OS Take Control</a></h3>
<ul>
<li>The OS uses a timer interrupt; A device that's programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted and a pre-configured interrupt handle in the OS runs. </li>
<li>OS regained control of the CPU and it can do what it pleases</li>
<li>On boot time the OS tells the hardware which code to run when a timer interrupts happens. </li>
<li>On boot time the OS must start the timer</li>
</ul>
<h3 id="saving-and-restoring--context"><a class="header" href="#saving-and-restoring--context">Saving and restoring  context</a></h3>
<p>The OS has regained control and now has a decision to make: </p>
<ul>
<li>To continue running the currently-running process </li>
<li>To switch to a different process</li>
</ul>
<h4 id="switch-to-a-different-process"><a class="header" href="#switch-to-a-different-process">Switch to a different process</a></h4>
<p>If the decision if to switch to a different process, the kernel executes a low level piece of code known as a context switch: </p>
<ul>
<li>The OS will execute some low-level assembly to save :
<ul>
<li>General purpose registers </li>
<li>PC (program counter)</li>
<li>Kernel stack pointer of the currently running process</li>
</ul>
</li>
<li>The OS will restore
<ul>
<li>General purpose registers</li>
<li>PC (program counter)</li>
<li>Switch to kernel stack of soon to be run process
By switching stacks, the kernel does the context switch. </li>
</ul>
</li>
</ul>
<p>When the OS finally execute a return-from-trap instruction, the soon-to-be-executing process become the currently running process, and thus the context switch is complete. </p>
<center><img src="./images/ContextSwitch.png"></center>
<div style="break-before: page; page-break-before: always;"></div><h1 id="4-scheduling"><a class="header" href="#4-scheduling">4. Scheduling</a></h1>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<h3 id="turnaround-time"><a class="header" href="#turnaround-time">Turnaround time:</a></h3>
<ul>
<li>Defined as <code>time of completion</code> - <code>time of arrival</code> </li>
<li>T = TCompletion - TArrival</li>
</ul>
<h3 id="response-time"><a class="header" href="#response-time">Response time</a></h3>
<ul>
<li>Time of response = Time first run minus time of arrival. </li>
<li>T = TFirstRun - TArrival</li>
</ul>
<h2 id="fifo"><a class="header" href="#fifo">FIFO</a></h2>
<ul>
<li>First in first out: The first process to arrive gets executed first. </li>
<li>Problems: 
<ul>
<li>Given 3 processes: A, B and C (which arrives at that order). A runs for 100 seconds and B and C for 10 seconds each. </li>
<li>Time of completion for: A = 100 secs, B = 110 and C = 120. Average 110 secs. </li>
<li><strong>FIFO behaves poorly on processes of different lengths</strong> </li>
</ul>
</li>
</ul>
<h2 id="shortest-job-first-sjf"><a class="header" href="#shortest-job-first-sjf">Shortest Job First (SJF)</a></h2>
<ul>
<li>The shorts job runs first, then the next one and so on. </li>
<li>Average turn around time for process A, B and C decreases from 110 secs to 50 secs. </li>
<li><strong>Problems</strong> 
<ul>
<li>If A arrives first and 10 seconds later B and C arrive, we will get similar turnaround time than in FIFO. </li>
</ul>
</li>
</ul>
<h2 id="shortest-time-to-completion-first-stcf"><a class="header" href="#shortest-time-to-completion-first-stcf">Shortest time to completion first (STCF)</a></h2>
<ul>
<li>The shortest job to completion runs.</li>
<li>If process A which takes 100 seconds it's running and a process B which takes 5 seconds arrives 10 seconds later A starts running, we switch to run process B because it will end before.</li>
<li>Bad response time. </li>
</ul>
<h2 id="round-robin"><a class="header" href="#round-robin">Round Robin</a></h2>
<ul>
<li>Runs a jobs for a time slice</li>
<li>The time slice must be a multiple of the timer-interrupt period</li>
<li>Better response time</li>
<li>Shorter time slice makes RR perform better on the response time metric </li>
<li>Too short makes context switching slow down the systems</li>
</ul>
<hr />
<ul>
<li>Fair scheduling policies perform poorly on metrics like turn around time but good on metrics like response time. (SJF, STCF)</li>
<li>Unfair scheduling policies perform poorly on response time and better on turn around time.  (RR)</li>
</ul>
<h2 id="io"><a class="header" href="#io">I/O</a></h2>
<ul>
<li>If process A  has I/O operations: |AAAAA|I/O|AAAAA|
<ul>
<li>It spits the process in two, the I/O it's the delimiter</li>
</ul>
</li>
<li>When the I/O is in progress another process can run (i.e B)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="5-multi-level-feedback-queue"><a class="header" href="#5-multi-level-feedback-queue">5. Multi-Level Feedback Queue</a></h1>
<h2 id="goals"><a class="header" href="#goals">Goals:</a></h2>
<ul>
<li>Minimize turn around time (Time of completion - Time of arrival)
<ul>
<li>Problems:
<ul>
<li>Minimizing the turnaround time is usually done by running the shortest job first</li>
<li>We don't know how long a process will take to run. </li>
</ul>
</li>
</ul>
</li>
<li>Minimize response time
<ul>
<li>Problems: 
<ul>
<li>Usually minimizing response time results in a bad turnaround time</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="crux-how-to-schedule-without-perfect-knowledge"><a class="header" href="#crux-how-to-schedule-without-perfect-knowledge">Crux: How to schedule without perfect knowledge</a></h4>
<h2 id="mlfq-basic-rules"><a class="header" href="#mlfq-basic-rules">MLFQ: Basic rules</a></h2>
<p>We have a number of queues, each of one has a different priority. At any give time, a job that is ready to run a job is <strong>on a single</strong> queue. The scheduler chooses the highest priority job to run, if there are many jobs with the same priority, Round Robin it's used on that queue. </p>
<ol>
<li><strong>Rule 1:</strong>  If Priority(A) &gt; Priority(B), A runs (B doesn't)</li>
<li><strong>Rule 2:</strong> If priority (A) == Priority(B), A &amp;&amp; B run in RR</li>
</ol>
<ul>
<li>MLFQ varies process priority based on observed behavior: 
<ul>
<li>If process yields CPU control while waiting or input keyboard, it get's higher priority (short execution times). </li>
</ul>
<ul>
<li>If process uses the CPU for long times without I/O, it get's lower priority. </li>
</ul>
<ul>
<li>Current problems: Highest priority jobs will always get to run, and lowest priority jobs will never run. </li>
</ul>
</li>
</ul>
<h3 id="1-change-priority"><a class="header" href="#1-change-priority">1. Change priority</a></h3>
<ol>
<li><strong>Rule 3:</strong> When a job enters the system, it is placed at the highest priority.</li>
<li><strong>Rule 4a:</strong> If a job uses his entire time slice while running, it's priority it's reduced</li>
<li><strong>Rule 4b:</strong> If a jobs gives up the CPU before the time slice is up, it stays at the same priority level.</li>
</ol>
<ul>
<li>Current problems with MLFQ:
<ul>
<li>Starvation: If there are too many high priority processes (short running processes), then they'll use all the CPU time and the long-running jobs will never run. </li>
</ul>
<ul>
<li>A user could trick to make the scheduler think our process is a short job by doing an I/O operation just before the time slice is over, thus keeping our job on the same priority queue.
- A program may change its behavior over time; what was CPU bound may transition to a phase of interactivity. </li>
</ul>
</li>
</ul>
<h3 id="2-priority-boost"><a class="header" href="#2-priority-boost">2. Priority boost</a></h3>
<p>To avoid the problem of starvation, we could periodically boost the priority of all the jobs in system. </p>
<ol>
<li><strong>Rule 5:</strong> After some time period <em>S</em>, move all the jobs in the system to the topmost queue 
Problems that solves this rule: </li>
</ol>
<ul>
<li>By sitting on the top queue our process are guaranteed not to starve because all jobs in the queue share the CPU in a round-robin fashion.</li>
<li>If a CPU-bound job has become interactive, the scheduler treats it properly once it has received the priority boost. 
<img src="./images/PriorityBoost.png" alt="Priority Boost image" /></li>
</ul>
<p>On the left, without priority boost, process in black get's starved once the short jobs arrive to the CPU, on the right however, with a priority boost every 50ms, the long running process it's guarantee to make some progress because it get's boosted to the top queue every 50ms and thus getting to run periodically. </p>
<h4 id="how-to-set-the-time-period-s-"><a class="header" href="#how-to-set-the-time-period-s-">How to set the time period <em>S</em> ?</a></h4>
<p>These values are called voo-doo constants because some form of <em>black magic</em> is needed to set them correctly. </p>
<ul>
<li>If <em>S</em> is set too high, long running jobs could starve. </li>
<li>If <em>S</em> is set too low, interactive jobs mayt not get a proper share of the CPU.</li>
</ul>
<h3 id="3-better-accounting"><a class="header" href="#3-better-accounting">3. Better accounting</a></h3>
<p>How we prevent gaming of our scheduler? We set a time max of how much a process can run, by <em>remembering</em> how long has he run and making the total sum, if it's greater than the max, we decrease his priority. 
Hence we rewrite rules <em>4a</em> and <em>4b</em>: </p>
<ol>
<li><strong>Rule 4:</strong> Once a job uses up its time allotment at a given level (regardless of how many time it has give up the CPU), it's priority it's reduced. (i.e., it moves down one queue) </li>
</ol>
<h2 id="summary"><a class="header" href="#summary">Summary:</a></h2>
<p>MLFQ has multiple levels of queue and uses feedback to determine the priority of a given job. History is its guide: Pay attention to how jobs behave over time and treat them accordingly. 
Refined set of MLFQ rules:</p>
<ul>
<li><strong>Rule 1:</strong> If Priority(A) &gt; Priority(B), A runs (B doesn't) </li>
<li><strong>Rule 2:</strong> If Priority(A) = Priority(B), A &amp; B run in round-robin fashion using the time slice of the given queue. </li>
<li><strong>Rule 3:</strong> When a job enters the system, it is placed at the highest priority. </li>
<li><strong>Rule 4:</strong> Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced.</li>
<li><strong>Rule 5:</strong> After some time period <em>S</em>, move all the jobs in the system to the topmost queue. </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="6-scheduling---proportional-share"><a class="header" href="#6-scheduling---proportional-share">6. Scheduling - Proportional Share</a></h1>
<p>Based around simple concept: <strong>Instead of optimizing for [[4. Scheduling#Turnaround time|turnaround]] or [[4. Scheduling#Response time|response]] time, the scheduler tries to guarantee that each job obtain a certain percentage of CPU time.</strong></p>
<h2 id="lottery-scheduling"><a class="header" href="#lottery-scheduling">Lottery scheduling</a></h2>
<p>Every so often, hold a lottery to determine which process should get to run next; processes that should run more often should be given more changes to win the lottery. </p>
<h3 id="tickets-represent-your-share"><a class="header" href="#tickets-represent-your-share">Tickets represent your share</a></h3>
<p>Tickets are used to represent the share of a resource that a process should receive. i.e. If we have two processes A and B, where a has 75 tickets and B has 25 tickets, that would mean that A should receive 75% of the CPU and B 25% of the CPU. 
The longer these two jobs compete, the more likely they are to achieve the desired percentages.</p>
<h3 id="ticket-mechanisms"><a class="header" href="#ticket-mechanisms">Ticket mechanisms</a></h3>
<h4 id="ticket-currency"><a class="header" href="#ticket-currency">Ticket currency</a></h4>
<p>It allow user to allocate tickets among their own jobs in whatever currency hey would like; the system then automatically converts said currency into the correct global value</p>
<p><strong>Example:</strong> User A has two jobs, A1 and A2; User B has 1 job, B1. The operating systems gives user A and user B 100 tickets each and user A gives A1 500 tickets and A2 500 tickets while user B gives 1000 tickets to B1.</p>
<pre><code>User A -&gt; 500  (A's currency) to A1 -&gt; 50  (global currency)
       -&gt; 500  (A's currency) to A2 -&gt; 50  (global currency)
User B -&gt; 1000 (B's currency) to B1 -&gt; 100 (global currency)
</code></pre>
<h4 id="ticket-transfer"><a class="header" href="#ticket-transfer">Ticket transfer</a></h4>
<ul>
<li>A process can temporarily hand off its tickets to another process</li>
</ul>
<h4 id="ticket-inflation"><a class="header" href="#ticket-inflation">Ticket inflation</a></h4>
<ul>
<li>A process can temporarily raise or lower the number of tickest it owns. </li>
<li>Only valid on scenarios where processes trust one another. </li>
<li>If any one process knows it needs more CPU time, it can boost its ticket value as a way to reflect that need to the system, without needing to communicate it to the other processes.</li>
</ul>
<h3 id="how-to-assign-tickets"><a class="header" href="#how-to-assign-tickets">How to assign tickets</a></h3>
<p>We could assume that the users know best; in such case, each user is handed some number of tickets, and a user can allocate tickets to nay job they run as desired. 
-&gt; That's a non solution since we are basically passing the problem to the user.</p>
<h2 id="stride-scheduling"><a class="header" href="#stride-scheduling">Stride scheduling</a></h2>
<p>Deterministic fair-share scheduler. </p>
<ul>
<li>Stride: Number that's inverse in proportion to the number of tickets a process has.</li>
<li>Each job in the system has a stride. </li>
<li>The more tickets, the lower stride. </li>
<li>Each process has an initial pass value. </li>
<li>Pick the process to run that has the lowest pass value so far</li>
<li>When you ran a process, increment its pass counter by its stride.</li>
</ul>
<p>A simple pseudocode: </p>
<pre><code class="language-C">curr = remove_min(queue);   // pick client with min pass
schedule(curr);             // run for quantum
curr-&gt;pass += curr-&gt;stride; // update pass using stride
insert(queue, curr);        // return curr to queue
</code></pre>
<p>With lower stride value, you will run the process more times. </p>
<ul>
<li>A stride scheduling cycle is when all pass value are equal. </li>
<li>At the end of each cycle, each process will have run in the same proportion to their ticket values. </li>
</ul>
<h3 id="main-problem-with-stride-scheduling"><a class="header" href="#main-problem-with-stride-scheduling">Main problem with stride scheduling</a></h3>
<ul>
<li>If a new job enters in the middle of our stride scheduling, what should its pass value be? Should it be set to 0? This will monopolize the CPU...</li>
<li>With [[6. Scheduling - Proportional Share#Lottery scheduling|Lottery Scheduling]] this doesn't happen, if a new process arrives, we just increase the total number of tickets and we go on to the next cycle. </li>
</ul>
<h2 id="linux-completely-fair-scheduler-cfs"><a class="header" href="#linux-completely-fair-scheduler-cfs">Linux Completely Fair Scheduler (CFS)</a></h2>
<p>Goal: To fairly divide a CPU evenly among all competing processes.
To achieve this goal, it uses a simple counting-based technique know as virtual runtime (<code>vruntime</code>)</p>
<ul>
<li>As each process runs, it accumulates <code>vruntime</code></li>
<li>Most basic case: Each process's <code>vruntime</code> increases at the same rate, in proportion with real time. </li>
<li>Scheduling decision: CSF will pick the process with the lowest <code>vruntime</code></li>
<li>How to know when to switch? 
<ul>
<li>CSF switches too often: Fairness increases, costs performance. </li>
<li>CSF switches less often: Near-term fairness, performance increases </li>
<li><code>sched_latency</code> is the value CFS uses to determine how long one process should run before considering a switch. CFS divides this value by the number of (<em>n</em>) processes running on the CPU to determine the time slice for a process, ensuring that over this period of time, CFS will be completely fair. </li>
</ul>
</li>
<li>How it works: 
<ul>
<li>We have <em>n</em> = 4 processes running, CFS divides the value of <code>sched_latency</code> (i.e. 48 ms) by <em>n</em>  to arrive arive at a per-process time slice of 12 ms. </li>
<li>CSF schedules the first job and runs it until it has used 12 ms of (virtual) runtime.</li>
<li>Checks to see if there is a job with lower <code>vruntime</code> to run instead</li>
<li>In this case there is, CFS switches to any of the other 3 jobs </li>
<li>Repeat. </li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="7-address-spaces"><a class="header" href="#7-address-spaces">7. Address Spaces</a></h1>
<p>Early machines didn't provide much of an abstraction of memory to users, the OS sat in memory (i.e. at address 0) and there would be one running program that currently sat in physical memory and used the rest of memory. </p>
<h2 id="multiprocessing-and-time-sharing"><a class="header" href="#multiprocessing-and-time-sharing">Multiprocessing and time sharing</a></h2>
<p>What we do is leave processes in memory while switching between them, allowing the OS to implement time sharing efficiently. </p>
<p>In the diagram, there are three processes (A, B and C) and each of them have a small part of the 512KB physical memory carved out for them. Assuming a single CPU, the OS chooses to run one of the processes (i.e A), while the others sit in the ready queue waiting to run. </p>
<center><img src="./images/VirtualMem.png"></center>
<p>This creates an <strong>issue</strong>: You don't want a process to access or modify memory of other processes. </p>
<h2 id="the-address-space"><a class="header" href="#the-address-space">The address space</a></h2>
<ul>
<li>The address space is the running program (process) view of memory in the system</li>
<li>Contains all of the memory state of the running program. This is:
<ul>
<li>The code of the program</li>
<li>Stack to keep track of where it is in the function call chain</li>
<li>Stack to allocate local variables and pass parameters and return values to and from routines</li>
<li>Heap used for dynamically allocated user-managed memory</li>
</ul>
</li>
</ul>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<center><img src="./images/AddressSpace.png"></center>
<ul>
<li>Here we have the program code at the top of the address space, this is because the code has a static size and wont grow. </li>
<li>Next the heap is located right after the code</li>
<li>The stack is located at the bottom of the address space</li>
<li>This placement of both the heap and the stack is to allow them to grow in size while the program is running. </li>
<li>The heap grows positively and the stack negatively</li>
<li><strong>This address space is an abstraction provided by the OS</strong>, the program isn't really at memory address 0 through 16KB; it is loaded at some arbitrary physical address(es) </li>
</ul>
<h2 id="crux-how-to-virtualize-memory"><a class="header" href="#crux-how-to-virtualize-memory">Crux: How to virtualize memory</a></h2>
<p>How can the OS build the abstraction of private, potentially large address space for multiple running processes on top of a single, physical memory? </p>
<h2 id="goals-1"><a class="header" href="#goals-1">Goals</a></h2>
<p>What the OS should have to implement memory virtualization the right way? </p>
<ul>
<li><strong>Transparency:</strong> OS should implement virtual memory in a way that's invisible to the running program. The program shouldn't know that is working on virtualize memory. </li>
<li><strong>Efficiency:</strong> OS should make virtualization: 
<ul>
<li>Time efficient</li>
<li>Space efficient (not using to much memory to implement virtualization)</li>
</ul>
</li>
<li><strong>Protection:</strong> OS should protect processes from one another, one process should not be able to to access or affect the memory of another process. </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="8-mechanism---address-translation"><a class="header" href="#8-mechanism---address-translation">8. Mechanism - Address Translation</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p><strong>Definition:</strong> With address translation, the hardware transforms each memory access(e.g, an instruction fetch, load, or store) changing the virtual address provided by the instruction to a physical address where the desired information is actually located. </p>
<ul>
<li>With every memory reference, a translation is done. </li>
<li>Hardware cannot do this by it's own, OS must manage memory (gets involved so the correct translations take place)</li>
<li>The main goal it's to create an illusion that the running program has it's own private memory, where only his code and data reside. </li>
</ul>
<h3 id="crux"><a class="header" href="#crux">Crux</a></h3>
<p>How to efficiently and flexibly virtualize memory? </p>
<h2 id="example-2"><a class="header" href="#example-2">Example:</a></h2>
<p>Suppose we have the following C program: </p>
<pre><code class="language-C">// simple.c
int main(void) {
	int x = 3000;
	x = x + 3;
	return 0;
}
</code></pre>
<p>To compile this code to assembly you can use <code>gcc</code>, on Linux: </p>
<pre><code class="language-bash">$ gcc -O0 -S -c simple.c -o simple.S
</code></pre>
<p><em>Note: the <code>-O0</code> part is highly important as it disables compiler optimization, without this, the compiler might realize you aren't doing nothing between <code>int x = 3000;</code> and <code>x = x + 3;</code> and might optimize that into <code>int x = 3003;</code></em></p>
<p>This will generate a <code>simple.S</code> assembly file which you can inspect. On <em>x86-64</em> look for two consecutive instructions <code>movl</code> and <code>addl</code>
The compiler turns this code into assembly, which <strong>might</strong> look like this (in <em>x86</em> assembly): </p>
<pre><code class="language-assembly">128: movl 0x0(%ebx), %eax
132: addl $0x03, %eax
135: movl %eax, 0x0(%ebx)
</code></pre>
<p>In this code snippet: </p>
<ul>
<li>Address of <code>x</code> has been placed in the register <code>ebx</code> </li>
<li>Value at <code>ebx</code> it's loaded to the general purpose register <code>eax</code> using <code>movl</code> instruction.</li>
<li><code>addl</code> adds 3 to <code>eax</code> </li>
<li>The final instruction stores the value in <code>eax</code> back into memory at that same location. </li>
</ul>
<p>The address space of this process might look like this: </p>
<center><img src="./images/ProcAS.png"></center>
<p>Here we can see: </p>
<ul>
<li>The 3 instruction code sequence is located at address <code>128</code></li>
<li>The value of the variable <code>x</code> is at address 15KB (on the stack)</li>
</ul>
<p>When the process run: </p>
<ul>
<li>Fetch instruction at address 128</li>
<li>Execute this instruction (load from address 15KB) </li>
<li>Fetch instruction at 132</li>
<li>Execute this instruction</li>
<li>Fetch instruction at 135</li>
<li>Execute this instruction (store to address 15KB)</li>
</ul>
<h3 id="notes"><a class="header" href="#notes">Notes</a></h3>
<ul>
<li>From this example program perspective, his address starts at 0 and ends at 16KB </li>
<li>However the OS might allocated the memory of this program somewhere else in memory (i.e between address 32KB and 48KB )</li>
<li>The size it's the same however the location differs. </li>
</ul>
<h2 id="dynamic-hardware-based-relocation"><a class="header" href="#dynamic-hardware-based-relocation">Dynamic (Hardware-based) Relocation</a></h2>
<ul>
<li>It's a technique to do hardware-based address translation. </li>
<li>We'll need two hardware registers within each CPU:
<ul>
<li>One is called <em>base</em> register</li>
<li>The other the <em>bounds</em> (a.k.a <em>limit</em> register)</li>
</ul>
</li>
<li>This registers allows us to place the address space of a program anywhere in physical memory </li>
<li>Also called <em>base-and-bounds</em></li>
</ul>
<h3 id="base"><a class="header" href="#base">Base</a></h3>
<p>Each program is written and compiled as if it is loaded at address zero, however when the program starts running, the OS decides where in physical memory the program is loaded and it set the registers to those values.</p>
<p>When a memory reference happens in the running program, the process translate the memory in the following manner: </p>
<pre><code>physical address = virtual address + base
</code></pre>
<p>For example, in the previous example, when this instruction has to be fetched: </p>
<pre><code>128: movl 0x0(%ebx), %eax
</code></pre>
<p>The cpu first adds the value of the base register (in this case 32KB) to the 128 to get 32896. This is the physical  address of the instruction and now the CPU can fetch and execute this instruction.</p>
<p>Same happens when accessing the value <code>3000</code> at address 15KB, the base register value is added to get physical address 47KB.</p>
<h3 id="bound"><a class="header" href="#bound">Bound</a></h3>
<p>The point of this register is to make sure that all addresses generated by the process are legan and within the <em>bounds</em> of the process. </p>
<ul>
<li>When a memory reference happens, the process will first check that the memory reference is <em>within bounds</em> (is legal), the bound limit helps with protection. </li>
<li>It can contain either the size of the address space or the physical address of the end of the address space. </li>
</ul>
<h3 id="example-translations"><a class="header" href="#example-translations">Example translations</a></h3>
<p>We have a process with an address space of size 4KB that has been loaded at physical address 16KB:</p>
<div class="table-wrapper"><table><thead><tr><th>Virtual address</th><th>Physical address</th></tr></thead><tbody>
<tr><td>0</td><td>16KB</td></tr>
<tr><td>1KB</td><td>17KB</td></tr>
<tr><td>3000</td><td>19384</td></tr>
<tr><td>4400</td><td><em>Fault (out of bounds</em></td></tr>
</tbody></table>
</div>
<h2 id="operating-system-issues"><a class="header" href="#operating-system-issues">Operating system issues</a></h2>
<p>There are a few critical junctures where the OS must get involved to implement our based-and-bounds version of virtual memory: </p>
<ol>
<li>The OS when a process is created must find space for its address space in memory. </li>
<li>When a process is terminated, it must reclaim all of its memory to be used by other process. Cleaning all data structures associated with that process</li>
<li>When a context switch occur, the OS must save and restore the base-and-bound pair when it switches between processes. </li>
<li>It must provide exception handlers. For example if a process tries to access memory outside of its bounds, the CPU will raise an exception; the OS must be prepared to take action when such an exception arises. </li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="9-segmentation"><a class="header" href="#9-segmentation">9. Segmentation</a></h1>
<p><em>Note: MMU = Memory management unit</em>
Problem with base-and-bound seen at <em>8</em> is that there is a big chunk of &quot;free&quot; space right in the middle, between the stack and the heap.
The simple approach of using base and bound register pair to virtualize memory is wasteful and it doesn't help us either when the memory of the program is bigger than the address space. </p>
<h2 id="crux-how-to-support-a-large-address-space"><a class="header" href="#crux-how-to-support-a-large-address-space">Crux: How to support a large address space</a></h2>
<p>How do we support a large address space with a lot of free space between the stack and the heap?</p>
<h2 id="segmentation-generalized-basebounds"><a class="header" href="#segmentation-generalized-basebounds">Segmentation: Generalized Base/Bounds</a></h2>
<p>Instead of having a base and bounds per address space, we have base and bound per logical segment of the address space. </p>
<ul>
<li>Segment: Contiguous portion of the address space of a particular length. In our case, we have code, stack and heap. </li>
<li>Segmentation allows us to place each one of this segments in different places in physical memory</li>
</ul>
<center><img src="./images/SegmentSpace.png"></center>
<p>In this figure we can see the different segments (Stack, Code and Heap) of a process. Each one is located independently of one another and only used memory is allocated space in physical memory, thus large address spaces with large amount of unused address space can be accommodated. </p>
<ul>
<li>The hardware structure in our MMU is a set of 3 base and bound register pairs:</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Segment</th><th>Base</th><th>Size</th></tr></thead><tbody>
<tr><td>Code</td><td>32K</td><td>2K</td></tr>
<tr><td>Heap</td><td>34K</td><td>3K</td></tr>
<tr><td>Stack</td><td>28K</td><td>2K</td></tr>
</tbody></table>
</div>
<h3 id="example-translation"><a class="header" href="#example-translation">Example translation</a></h3>
<center><img src="./images/SegmentAP.png"></center>
<p>If we have a program with the address space of the figure above and is placed on physical memory as the first figure illustrated. 
*Note: Offset is defined as the &quot;distance&quot; between the virtual address we are referencing and the start of the segment</p>
<ul>
<li>Reference virtual address 100 (Code): The hardware adds the <code>base</code> value (see first table) to the <em>offset</em> into this segment (100 in this case). Physical address: 100 + 32KB = 32868</li>
<li>Reference to virtual address 4200 (heap): The offset of this segment is 4200 minus 4KB because the heap starts at address 4K. Then we add the base value (34K) to the calculated <em>offset</em> (104) to get 34920</li>
<li>In general the operation is (For positive growing segments): </li>
</ul>
<pre><code>offset = (virtual mem value - segment start value virtual mem)
physical address = offset + segment base
</code></pre>
<ul>
<li>For the heap: 
<ul>
<li>Segment start value at virtual memory = 4KB</li>
<li>Virtual mem value (the address we want to reference) = 4200</li>
<li>Base heap value = 34K</li>
</ul>
<ul>
<li>Physical address = (4200 - 4096) + 34816 = 34920</li>
</ul>
</li>
</ul>
<h2 id="which-segment-are-we-referring"><a class="header" href="#which-segment-are-we-referring">Which segment are we referring</a></h2>
<p>Explicit approach chop up the address space in segments based on the top few bits of the virtual address. If our example we have 3 segments so we need 2 bits to represent each one, hence we will use the top 2 bits of our 14-bit virtual address to select the segment. </p>
<center><img src="./images/SegmentExplicit.png"></center>
<ul>
<li>
<p>If the top bits are 00, the virtual address is in the code segment, if it's 01 then it's in the heap segment.</p>
</li>
<li>
<p>If we wanted to refer to virtual address 4200, which can be seen in binary form as: </p>
</li>
</ul>
<center><img src="./images/addressExample.png"></center>
<ul>
<li>
<p>We see that our segment is 01 (Heap) on our offset value in decimal is 104. We add the base register to the offset and we get the physical address</p>
</li>
<li>
<p>If base and bounds where arrays, the hardware would be doing something like this to obtain the desired physical address </p>
</li>
</ul>
<pre><code class="language-C">// Get top 2 bits of 14-bit VA
Segment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT
/// Now get the offset
Offset = VirtualAddress &amp; OFFSET_MASK
if (Offset &gt;= Bounds[Segment])
	RaiseException(PROTECTION_FAULT)
else
	PhysAddr = Base[Segment] + Offset
	Register = AccessMemory(PhysAddr)
</code></pre>
<h4 id="issues"><a class="header" href="#issues">Issues:</a></h4>
<ul>
<li>When we use the top two bits and only have 3 segments, one segment of the address space goes unused. </li>
<li>It limits use of the virtual address space, each segment is limited to a maximum size (4KB in this case because that's the max number that can be represented using 12 bits = 14 bits - 2 top bits). If we want a bigger segment we are out of luck. </li>
</ul>
<h4 id="possible-fixes"><a class="header" href="#possible-fixes">Possible fixes:</a></h4>
<ul>
<li>Implicit approach: The hardware determines the segment by noticing how the address was formed. </li>
</ul>
<h2 id="the-stack"><a class="header" href="#the-stack">The stack</a></h2>
<p>In our first diagram, we can see that our stack is located at physical memory 28KB but with one critical difference: <em>it grows backwards</em></p>
<ul>
<li>In physical memory it starts at 28KB and grows back to 26KB</li>
</ul>
<ul>
<li>Corresponding to virtual address 16KB to 14KB</li>
</ul>
<p>To keep track of the direction a segment grows we need help from the hardware (register set to 0 indicates negatives grow and 1 indicates positive grow)</p>
<div class="table-wrapper"><table><thead><tr><th>Segment</th><th>Base</th><th>Size</th><th>Grows Positive?</th></tr></thead><tbody>
<tr><td>Code</td><td>32K</td><td>2K</td><td>1</td></tr>
<tr><td>Heap</td><td>34K</td><td>3K</td><td>1</td></tr>
<tr><td>Stack</td><td>28K</td><td>2K</td><td>0</td></tr>
</tbody></table>
</div>
<h3 id="stack-address-translation"><a class="header" href="#stack-address-translation">Stack address translation</a></h3>
<p>In this example we want to access virtual address 15KgB</p>
<ul>
<li>The stack segment starts at 16KB, hence the offset is 15KB - 16KB = -1KB</li>
<li>The base of the stack is at physical address 28KB, hence the address we want to access is at -1KB + 28KB = 27KB </li>
<li>Bound is check by checking that the absolute value of the negative offset (1KB in this case) is less than or equal to the segment's current size (2KB in this case)</li>
</ul>
<h2 id="support-for-sharing"><a class="header" href="#support-for-sharing">Support for sharing</a></h2>
<p>We can also add support for sharing segments of memory between address spaces.</p>
<ul>
<li>We need extra hardware support in the form of protection bits. </li>
<li>Protection bits: Adds a few bits per segment indicating if a segment can be read, write and/or executable</li>
<li>If you set a code to read only, you can share it across multiple processes, without worry of harming isolation. 
With this, we now have the following segment registers values:</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Segment (Top bit)</th><th>Base</th><th>Size</th><th>Grows Positive?</th><th>Protection</th></tr></thead><tbody>
<tr><td>Code (<code>00</code>)</td><td>32KB</td><td>2KB</td><td>1</td><td>Read-execute</td></tr>
<tr><td>Heap (<code>01</code>)</td><td>34KB</td><td>3KB</td><td>1</td><td>Read-Write</td></tr>
<tr><td>Stack (<code>11</code>)</td><td>28KB</td><td>2KB</td><td>0</td><td>Read-Write</td></tr>
</tbody></table>
</div>
<ul>
<li>With this, the hardware algorithm seen before now has to check that the segment where that a virtual memory is referencing, has the proper permissions (read, write or execute) </li>
</ul>
<h2 id="fine-grained-vs-coarse-grained-segmentation"><a class="header" href="#fine-grained-vs-coarse-grained-segmentation">Fine-grained vs. Coarse-grained Segmentation</a></h2>
<ul>
<li>Segmentation with a few segments (i.e just Code, Heap and Stack): coarse-grained</li>
<li>Coarse grained segmentation: Chops up the address space into relatively large coarse chunks</li>
<li>In contrast: Large number of smaller segments is referred to as fine-grained segmentation.</li>
<li>To support fine-grained segmentation we need even further hardware support with a segment table of some kind stored in memory.</li>
</ul>
<h2 id="os-support"><a class="header" href="#os-support">OS Support</a></h2>
<h4 id="segmentation-raises-a-number-of-new-issues-for-the-operating-system"><a class="header" href="#segmentation-raises-a-number-of-new-issues-for-the-operating-system">Segmentation raises a number of new issues for the operating system:</a></h4>
<ol>
<li>What should the OS do on a context switch? Segment registers must be saved some place in memory and restored. </li>
<li>OS interaction when segments grow (or shrink). If a program calls <code>malloc()</code> to allocate space on the heap, but there isn't enough available memory to allocate this, the heap segment must grow, in this case, the memory-allocation library will perform a system call to grow the heap (i.e. <code>sbrk()</code> on Linux), the OS would then grow the segment size and updating the segment size registers to the new size, and informing the library of success (or failure if there isn't enough space or an arbitrary limit has been reached.)</li>
<li>Managing free space in physical memory. When a new process is created we have to allocate the address space in physical memory, however, the size of each segment is not constant now. This arises a problem, physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments (this problem is known as <strong>external fragmentation</strong> )</li>
</ol>
<h4 id="possible-solutions"><a class="header" href="#possible-solutions">Possible solutions:</a></h4>
<ol>
<li>Compact physical memory: By rearranging the existing segments, we compact the physical memory. If a process segment doesn't have enough space in his current position at the physical memory, move it entirely to new part of the physical memory, updating the segment registers. 
<ul>
<li>This is expensive (CPU and Memory wise) </li>
<li>Uses a fair amount of processor time</li>
</ul>
</li>
<li>Use a free-list management algorithm that tries to keep large extents of memory available for allocation (There are hundred of this kind of algorithms)</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="10-free-space-management"><a class="header" href="#10-free-space-management">10. Free-Space Management</a></h1>
<ul>
<li>It becomes more difficult when the free space you are managing consist of variable sized units</li>
<li>Usually arises in user-level memory-allocation libraries (i.e. <code>malloc()</code> and <code>free()</code>)</li>
<li>Arises in the OS when using segmentation to implement virtual memory</li>
<li>Problem known as external fragmentation: The free space gets chopped into little pieces of different sizes and is thus fragmented; request may fail because there is no contiguos space of memory. </li>
</ul>
<p>Example: 
In this image we see that the total available space is 20 bytes, however if a process request 15 bytes it will fails since there's not contiguos space of memory that adds up to 15 bytes. </p>
<center><img src="./images/externFrag.png"></center>
<h2 id="assumptions"><a class="header" href="#assumptions">Assumptions</a></h2>
<p>We have a basic allocation library that has the following functions: 
- <code>void *malloc(size_t size)</code> where <code>size</code> is the number of bytes request by the application, it hands back a point (void pointer) to a region with the requested size. 
- <code>void free(void *ptr)</code> takes a single pointer and frees the corresponding chunk that the pointer is pointing to. 
2. The space this library manages is known as the heap, the generic data structured used to manage free space in the heap is some kind of free list
3. Free list containers references to all of the free chunks of space in the managed region of memory. 
4. We are primarily concerned with external fragmentation
5. When a virtual memory is handed out to a client, it cannot be relocated to another location in memory. The region where the pointer is pointing to wont be relocated. 
6. The allocator manages a contiguous region of bytes. For this case in particular we assume that the region is a single fixed size throughout its life. </p>
<h2 id="low-level-mechanisms"><a class="header" href="#low-level-mechanisms">Low level Mechanisms</a></h2>
<h3 id="splitting-and-coalescing"><a class="header" href="#splitting-and-coalescing">Splitting and Coalescing</a></h3>
<p>We have the following 30 bytes heap: </p>
<p>![[./images/bytesheap.png]]</p>
<p>Assuming we have the following free list: </p>
<pre><code>head -&gt; {addr: 0, len: 10} -&gt; {addr: 20, len: 10} -&gt; NULL
</code></pre>
<p><strong>Splitting:</strong> If we have a request for a single bytes, the allocator performs this action (Splitting). It finds a free chunk of memory that can satisfy the request and split it in two, the first part will be returned to the caller, the second chunk will remain on the list.
For example if we choose the second element on the free list, we will end up with this free list after the allocation: </p>
<pre><code>head -&gt; {addr: 0, len: 10} -&gt; {addr: 21, len: 9} -&gt; NULL
</code></pre>
<p>Here we return the address <code>20</code> to the caller, and the start of the second element would be then <code>21</code> instead of <code>20</code>
Splitting is usually used when the requested sized is smaller than the size of any particular free chunk. </p>
<p><strong>Coalesce</strong> Free space when a chunk of memory is freed. 
Imagine we free the memory that's on the middle of our heap, we would then have the following free list: </p>
<pre><code>head -&gt; {addr: 10, len: 10} -&gt; {addr: 0, len: 10} -&gt; {addr: 20, len: 9} -&gt; NULL
</code></pre>
<p>And when a user requests, for example, 20 bytes, we won't be able to find a 20 bytes chunk even tho the first and second chunk are neighbors and could be used as a 20 bytes chunk. 
To fix this problem we use <strong>coalesce</strong>: When freeing a chunk of memory, check if the newly freed space sits right next to one existing free chunk, if it does, merge them into a single large chunk. </p>
<h3 id="tracking-size-of-allocated-regions"><a class="header" href="#tracking-size-of-allocated-regions">Tracking size of allocated regions</a></h3>
<ul>
<li>To track the size of allocated regions, we add a header to the top of the request memory, for example if we request 20 bytes <code>malloc(20)</code></li>
</ul>
<p>![[./images/headermalloc.png]]</p>
<ul>
<li>The header struct may look like this: </li>
</ul>
<pre><code class="language-C">typedef struct {
	int size;
	int magic;
} header_t;
</code></pre>
<ul>
<li>Here magic can be used to detect memory corruptions</li>
<li><code>size</code> is used to save the <code>size</code> of the allocated region</li>
<li>When we free this space, the total size would be <code>size</code> + the size of the header</li>
</ul>
<h2 id="strategies-for-managing-free-space"><a class="header" href="#strategies-for-managing-free-space">Strategies for managing free space</a></h2>
<h3 id="best-fit"><a class="header" href="#best-fit">Best fit</a></h3>
<ul>
<li>Search for memory chunks on the free list that can hold the requested size.</li>
<li>Select the smallest from the resulting chunks that can hold the requested size. </li>
<li>Literally the best fit. </li>
<li>Full search list is required, hence bad performance</li>
</ul>
<h3 id="worst-fit"><a class="header" href="#worst-fit">Worst fit</a></h3>
<ul>
<li>The oposite to best fit</li>
<li>Scan the free list and search for all possible chunks that can hold the requested size</li>
<li>Choose the biggest chunk</li>
<li>Full search list is required, hence bad performance
This strategies tries to keep big chunks free instead of lots of small chunks (which is what the best fit strategies does)</li>
</ul>
<h3 id="first-fit"><a class="header" href="#first-fit">First fit</a></h3>
<ul>
<li>Search for the first memory chunk that can hold the request size</li>
<li>Better speed as it stops as soon as it finds a chunk that can hold the requested size</li>
<li>Pollutes the beginning of the free list with small chunks, this can be mitigated by using address-based ordering list</li>
</ul>
<h3 id="next-fit"><a class="header" href="#next-fit">Next fit</a></h3>
<ul>
<li>Instead of starting at the beginning of the list like <strong>first fit</strong> does, we keep an extra pointer that holds the location we where last looking. </li>
<li>From that location, we start the first fit strategies </li>
<li>Performance similar to first fit</li>
<li>We avoid the pollution at the beginning of the list</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="11-paging---introduction"><a class="header" href="#11-paging---introduction">11. Paging - Introduction</a></h1>
<p><em>Basic definition: We chop up space into fixed-sized pieces, this is known as paging</em></p>
<ul>
<li>We divide a process's address space into fixed-sized units, each of which we call a page</li>
<li>Physical memory is view as an array of fixed-sized slots called page frames</li>
<li>Each page frame contains a single virtual-memory page</li>
</ul>
<h2 id="example-3"><a class="header" href="#example-3">Example</a></h2>
<p>Imagine we have a small address space of 64 bytes, with four 16-bytes pages. </p>
<center><img src="./images/pageas.png"></center>
<center><i>This could be the address space of a process, which is divided into 4 pages</i></center>
<p>And that we have physical memory which consists of fixed-size slots, in this case 8 page frames.</p>
<center><img src="./images/paginpm.png"></center>
<p>Here we have:</p>
<ul>
<li>A 128 bytes physical memory</li>
<li>8 page frames (from 0 to 7)</li>
<li>Each page of the address space mentioned at the beginning is located somewhere in memory (not necessarily in order) </li>
</ul>
<p><strong>Advantages of paging:</strong> </p>
<ul>
<li>Flexibility</li>
<li>Simplicity: If the OS wishes to place to place our 64 byte address space into our 8-page physical memory, it simply finds four free pages, perhaps the OS keeps a free list and just grabs the first 4 free page that finds</li>
</ul>
<p>Usually the OS keeps a per process data structure known as <strong>page table:</strong></p>
<ul>
<li>It store the address translation for each of the virtual pages of the address space of the process</li>
<li>This let us know where in physical memory a virtual page resides</li>
<li>In our example the page table would be something like this: </li>
</ul>
<pre><code>(Virtual Page 0 -&gt; Physical Frame 3)
(Virtual Page 1 -&gt; Physical Frame 7)
(Virtual Page 2 -&gt; Physical Frame 5)
(Virtual Page 3 -&gt; Physical Frame 2)
</code></pre>
<h3 id="example-memory-access"><a class="header" href="#example-memory-access">Example memory access</a></h3>
<ul>
<li>We have the address space described at the beginning</li>
<li>We want to perform a memory access: </li>
</ul>
<pre><code class="language-S">movl &lt;virtual address&gt;, %eax
</code></pre>
<p>Lets focus on how to translate the virtual address: </p>
<ul>
<li>We need to split it into two components:
<ol>
<li>Virtual page number (VPN) (This will help us map our virtual page number to a physical frame number)</li>
<li>The offset within the page</li>
</ol>
</li>
</ul>
<p>Since our virtual address space is 64 bytes, we need 6 bits total for our virtual address (2^6 = 64)
<em>This is because we can only reference 64 locations, and each location is 1 byte. We CAN'T reference a specific bit, the smallest addressable unit is a byte</em></p>
<p>Our virtual address can be seeing as this: </p>
<div class="table-wrapper"><table><thead><tr><th>Va5</th><th>Va4</th><th>Va3</th><th>Va2</th><th>Va1</th><th>Va0</th></tr></thead><tbody>
</tbody></table>
</div>
<p>Here the highest order bit is <code>Va5</code> and the lowest order bit is <code>Va0</code>. Also we know that the page size is 16 bytes, hence we only need 4 bits to address the virtual memory inside our page (2^4=16), finally our 2 remaining highest order bits can be used to know which page number are we in. </p>
<pre><code>[Va5, Va4] -&gt; VPN
[Va3, Va2, Va1, Va0] -&gt; Offset
</code></pre>
<p>If we know want to load, for example, virtual address 21:</p>
<pre><code class="language-S">movl 21, %eax
</code></pre>
<p>We know that 21 in binary is <code>010101</code>, hence we have the following virtual address: </p>
<p>![[./images/vaddresstable.png]]
We know that our <code>VPN = 01 = 1</code> and that our <code>offset = 0101 = 5</code>, hence our virtual address is on the first virtual page with offset of 5 bytes. </p>
<p>From our page table we know that virtual page <code>1</code> corresponds to <code>Physical Frame 7</code>, then we can replace the <code>VPN</code> with the <code>PFN</code> (Physical frame number) and then issue the load to physical memory: </p>
<center><img src="./images/translatepage.png"></center>
<p>The offset stays the same because it tells us which byte within the page we want to address. </p>
<h2 id="whats-in-the-page-table"><a class="header" href="#whats-in-the-page-table">What's in the page table</a></h2>
<ul>
<li>Page table is a data structure used to map virtual address to physical addresses, it a per process data structure, meaning each process has a page table</li>
<li>Simplest form: Linear page table (Just an array) </li>
<li>Linear page table indexes the array by the virtual page number and looks up the page-table entry (PTE) at that index to find the physical frame number (PFN). </li>
<li>For example if I wanted to translate the virtual page number 3 to physical frame number, I would access <code>array[3]</code>  array index 3. </li>
</ul>
<h3 id="whats-inside-a-page-table-entry-pte"><a class="header" href="#whats-inside-a-page-table-entry-pte">What's inside a Page-table entry (PTE)</a></h3>
<ul>
<li>
<p>Each PTE has a number of different bits worth understanding at some level.</p>
</li>
<li>
<p><strong>Valid bit</strong>: Indicates if a translation is valid. When a process start running, it will have stack and code at one end of his address space, and heap on the other end. All the space in between is unused hence it will be marked invalid. By marking all the unused pages in the address space, we remove the need to allocate physical frames for those pages, thus saving memory. </p>
</li>
<li>
<p><strong>Protection bits</strong>: Indicates if the page can be read, written or executed. </p>
</li>
<li>
<p><strong>Present bit</strong>: Indicates if the page is on physical memory or disk (swap on Linux)</p>
</li>
<li>
<p><strong>Dirty bit</strong>: Indicates whether the page has been modified since it was brought into memory</p>
</li>
<li>
<p><strong>Reference bit</strong>: Track whether a page has been accessed, it helps us to determine which pages are &quot;popular&quot; and thus should be kept in memory. </p>
</li>
</ul>
<p>An example page-table entry would look like this for the x86 architecture: </p>
<center><img src="./images/pte_example.png"></center>
<h2 id="the-problem-with-paging-too-slow"><a class="header" href="#the-problem-with-paging-too-slow">The problem with paging: Too slow</a></h2>
<p>Imagine we want to access memory <code>21</code> on our example's address space: </p>
<pre><code>movl 21, %eax
</code></pre>
<p>To do this we would need to translate virtual address space (21) into the physical address (117).
Hence, before fetching the data from address 117, our system needs to:</p>
<ul>
<li>Fetch the page table from the process's page table</li>
<li>Perform the trasnlation</li>
<li>Load the data from physical memory
To do so, the hardware must known where the page table is for the current process, let's imagines that this is on a cpu register, then our translate would look like this:</li>
</ul>
<pre><code>VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT
PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))

offset = VirtualAddress &amp; OFFSET_MASK
PhysAddr = (PFN &lt;&lt; SHIFT) | offset
</code></pre>
<ul>
<li>On our first line we take our virtual address and apply a <code>VPN_MASK</code> to keep only the bits that represent or virtual page number, on our example, with virtual address <code>21</code> we would have: 
<ul>
<li>21 in binary is <code>010101</code></li>
<li>Our <code>VPN_MASK</code> would be <code>110000</code> (we only need our most significant bits)</li>
<li><code>SHIFT</code> would be 4, because we want to move our bits 4 spaces to the right</li>
<li><code>VPN = (010101 &amp; 110000) &gt;&gt; 4 = (010000) &gt;&gt; 4 = (000001)</code></li>
</ul>
</li>
<li>The second line would take the memory address of the page table and add the index (<code>VPN</code> multiplied by the size of the entries in the table <code>sizeof(PTE)</code>) to get the page table entry address</li>
<li>Once the <code>PTEAddr</code> (page table entry address) is known, the hardware can load and extract the PFN (physical address of the page frame)</li>
<li>On the last line, we use the <code>PFN</code> and left shift it and append the <code>offset</code> to get the final physical address space</li>
<li>We can finally fetch the data from the physical address </li>
</ul>
<p>If you notice, we need to make one extra memory reference (we need to fetch the <code>PTE</code>) in order to get the initial memory address (<code>21</code> in this case).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="12-paging---faster-translations"><a class="header" href="#12-paging---faster-translations">12. Paging - Faster Translations</a></h1>
<p>On the previous chapter we saw that: </p>
<ul>
<li>By  using paging, we require a large amount of mapping information</li>
<li>The mapping information is usually stored in physical memory</li>
<li>Paging, hence requires an extra memory lookup for each virtual address generated by the running program (we need to check the mapped information that's stored in memory )</li>
</ul>
<h2 id="the-solution-tlb"><a class="header" href="#the-solution-tlb">The solution: TLB</a></h2>
<ul>
<li>To speed address translation we need help from the hardware</li>
<li>We add what's called a <strong>translation-lookaside buffer</strong> (TLB) </li>
<li>TLB is part of the chips memory-management unit (MMU) </li>
<li>It's a hardware cache, but because is &quot;closer&quot; to the CPU, memory access is much faster</li>
</ul>
<p><strong>What does it do?</strong> </p>
<ul>
<li>For each virtual memory reference, we check the TLB to see if the translation is there</li>
<li>If it is, we perform the translation, without having to access physical memory</li>
</ul>
<h2 id="tlb-basic-algorithm"><a class="header" href="#tlb-basic-algorithm">TLB Basic algorithm</a></h2>
<pre><code>VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT 
(Success, TlbEntry) = TLB_Lookup(VPN)
if (Success == True) // TLB Hit
	if (CanAccess(TlbEntry.ProtectBits) == True)
		Offset = VirtualAddress &amp; OFFSET_MASK
		PhysAddr = (TlbEntry.PFN &lt;&lt; SHIFT) | Offset
		Register = AccessMemory(PhysAddr)
	else
		RaiseException(PROTECTION_FAULT)
else // TLB Miss
	PTEAddr = PTBR + (VPN * sizeof(PTE))
	PTE = AccessMemory(PTEAddr) // Bad performance, we are accessing memory :(
	if (PTE.Valid == False)
		RaiseException(SEGMENTATION_FAULT)
	else if (CanAccess(PTE.ProtectedBits) == False)
		RaiseException(SEGMENTATION_FAULT)
	else
		TLB_Insert(VPN, PTE.PFN, PTE.ProtectedBits)
		RetryInstruction()	
</code></pre>
<p>This algorithms works like this: </p>
<ol>
<li>Extract the virtual page number (VPN) from the virtual address </li>
<li>Check if the TLB holds the translation for this VPN</li>
<li>If it does, then the TLB holds the translation, hence we can now:
<ol>
<li>Extract the page frame number (PFN) from the TLB</li>
<li>Concatenate the PFN onto the offset from the original virtual address, hence forming the desired physical address </li>
<li>Access memory, handle errors</li>
</ol>
</li>
<li>If it doesn't find the translation in the TLB:
<ol>
<li>Hardware accesses the page table to find the translation (this is costly)</li>
<li>Assuming that the translation is valid, we update the TLB</li>
<li>Once the TLB is updated, we hardware retries the instruction </li>
</ol>
</li>
</ol>
<h2 id="tlb-miss-handling"><a class="header" href="#tlb-miss-handling">TLB Miss handling</a></h2>
<p>There are two ways to handle a TLB miss: Hardware and software wise.</p>
<h3 id="hardware-handling"><a class="header" href="#hardware-handling">Hardware handling:</a></h3>
<p>To handle TLB miss, the hardware must know: </p>
<ul>
<li>Exactly where the page tables are located in memory</li>
<li>The exact format of the page tables
On miss, the hardware walks to the page table, find the correct page-table entry and extract the desired translation, updates the TLB with the translation and retry the instruction. </li>
</ul>
<h3 id="software-handling"><a class="header" href="#software-handling">Software handling</a></h3>
<p><strong>The basic process works like this</strong> </p>
<ul>
<li>On a miss, the hardware raises an exception</li>
<li>The exception pauses the current instruction stream, raises the privilege level to kernel mode, jumps to to a trap handler</li>
<li>The trap handler, is code within the OS that is written to handle the TLB misses</li>
<li>When the trap handler runs, it will lookup the translation in the page table, updates the TLB and return from the trap</li>
<li>The hardware retries the instruction </li>
</ul>
<p>Difference between the trap signal of the TLB miss and others we saw:</p>
<ul>
<li>The return from trap is different than the one we saw on system calls. On the case of system calls, we would simply resume execution after the trap into the OS, just like a return from a procedure call.</li>
<li>In the case of the TLB trap signal, we need to resumen execution on the instruction that caused the exception (basically running the instruction again), this can cause an infinite loop, so the OS needs to be careful when raising this exception. </li>
</ul>
<h2 id="tlb-contents"><a class="header" href="#tlb-contents">TLB Contents</a></h2>
<p>A TLB entry might look like this: </p>
<pre><code>VPN | PFN | other bits
</code></pre>
<ul>
<li>Both VPN and PFN are present in each entry.</li>
<li>A translation coudl end up in any of these locations (this is called a fully-associative cache)</li>
<li>The hardware searches the entries in parallel to see if there is a match</li>
</ul>
<p>The <code>other bits</code> might contain: </p>
<ul>
<li>A <strong>valid</strong> bit: If the entry has a valid translation or not</li>
<li><strong>Protection</strong> bit: Determines how a page can be accessed (for example, code page might be marked <em>read and execute</em> only, heap might be marked <em>read and write</em>, etc.)</li>
<li><strong>Address-space</strong>, <strong>identifier</strong>, <strong>dirty bit</strong> are also common bits present in here</li>
</ul>
<h2 id="tlb-issue-context-switches"><a class="header" href="#tlb-issue-context-switches">TLB Issue: Context switches</a></h2>
<p>When switching from one process to another, the hardware or OS must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process. </p>
<p><strong>Example:</strong> We have a running process (P1) that assumes that the TLB is caching translations that are valid for it, i.e, that come from P1's page table, let's assume that the 10th virtual page of P1 maps to hysical frame 100. Then a context switch happens to another process (P2), assume we also have a virtual page number for this process (P2) but instead it maps to physical frame 170, if entries of both process were in the TLB, the contents of the TLB would be something like this: </p>
<div class="table-wrapper"><table><thead><tr><th>VPN</th><th>PFN</th><th>valid</th><th>prot</th></tr></thead><tbody>
<tr><td>10</td><td>100</td><td>1</td><td>rwx</td></tr>
<tr><td>10</td><td>170</td><td>1</td><td>rwx</td></tr>
</tbody></table>
</div>
<p>Here we have a problem, two VPNs with the same value map to a different PFN, each VPN is valid only to their process but we don't have a way to tell from which process is each VPN</p>
<h3 id="possible-solutions-1"><a class="header" href="#possible-solutions-1">Possible solutions</a></h3>
<h4 id="flush-the-tlb-on-context-switch"><a class="header" href="#flush-the-tlb-on-context-switch">Flush the TLB on context switch</a></h4>
<ul>
<li>On software-based system: with an explicit hardware instruction</li>
<li>On hardware-managed TLB: The flush could be enacted when the page-table base register is changed</li>
<li>On either case, the flush operations sets all valid bits to 0, clearing the contents of the TLB</li>
<li><strong>Cost:</strong> Each time a process run, it must incur TLB misses as it touches its data and code pages, the more context switches are, the higher the cost. </li>
</ul>
<h4 id="address-space-identifier"><a class="header" href="#address-space-identifier">Address space identifier</a></h4>
<ul>
<li>Hardware support to enable sharing of the TLB is added via address space identifier (ASID)</li>
<li>The ASID if kinda like a process identifier, but it has fewer bits</li>
<li>If we take our example above, we would have the following table</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>VPN</th><th>PFN</th><th>valid</th><th>prot</th><th>ASID</th></tr></thead><tbody>
<tr><td>10</td><td>100</td><td>1</td><td>rwx</td><td>1</td></tr>
<tr><td>10</td><td>170</td><td>1</td><td>rwx</td><td>2</td></tr>
</tbody></table>
</div>
<ul>
<li>Here we can identify which VPN corresponds to which process using the ASID value. </li>
</ul>
<h2 id="tlb-issue-replacement-policy"><a class="header" href="#tlb-issue-replacement-policy">TLB Issue: Replacement policy</a></h2>
<p>When we are adding an entry, we have to replace it with an old one, which one to replace? 
Typical policies are: </p>
<ul>
<li>Least-recently-used (LRU)</li>
<li>Random policy</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="13-paging---smaller-tables"><a class="header" href="#13-paging---smaller-tables">13. Paging - Smaller Tables</a></h1>
<p>Assume we have the following system: </p>
<ul>
<li>32-bit address space (2\({^{32}}\) bytes)</li>
<li>4KB (2\({^{12}}\) bytes) pages</li>
<li>4-byte page-table entry
We then have roughly \(\frac{2^{32}}{2^{12}}\) virtual pages, that's \(1.048.576\), and considering that each page table entry is 4-bytes, we have \(1.048.576 * 4\)  bytes of memory used for the page table only, that is about 4MB in size. We usually have one page table for every process, that is a lot of memory usage for the page table only. How can we decrease this memory usage? </li>
</ul>
<h2 id="simple-solution-bigger-pages"><a class="header" href="#simple-solution-bigger-pages">Simple solution: Bigger pages</a></h2>
<p>If we increase our page table by 4 (that is from 4KB to 16KB), we then decrease or page table memory usage for a process from 4MB to 1MB. The reduction exactly mirrors the factor that we increase our page size. 
The main problems with this solutions is internal fragmentation, we are allocating too much memory for process that might not use all. </p>
<h2 id="hybrid-approach-paging-and-segments"><a class="header" href="#hybrid-approach-paging-and-segments">Hybrid approach: Paging and segments</a></h2>
<p>We combine paging and segmentation in order to reduce the memory overhead of pager tables. We can see why this might work by examining a typical linear page table in more detail: 
Assume we have: </p>
<ul>
<li>An address space with used portions of the heap and stack are small</li>
<li>A tiny 16KB address space with 1KB pages</li>
</ul>
<center><img src="./images/segment_example.png"></center>
<p>Here we have a virtual address space with a code, a heap and a stack segment. However we are only using 1 page on each segment. Page 0 (Code) maps to physical frame 10, Page 4 maps to 23, Page 14 maps to 4 and 15 maps to 28. Most of the pages on our address space are unused, this is a waste of space in our page table, because it stills has to storage 16 entries. </p>
<p><strong>Our hybrid approach:</strong> Instead of having a single page table for our entire virtual address space, we have a page table for each segment (1 table for code, 1 table for heap and 1 table for stack).</p>
<p>What each register will mean on our segment approach: </p>
<ul>
<li><strong>base</strong> register: Holds the physical address of the page table of that segment</li>
<li><strong>bound</strong> register: Used to indicate the end of the page table</li>
</ul>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<p>Assume: </p>
<ul>
<li>32-bits virtual address space </li>
<li>4KB pages</li>
<li>Address space split into four segments</li>
<li>We'll only use three segments for this example, code, heap and stack.</li>
<li>To determine a segment an address refers to, we'll use the top two bits of the address space: 
<ul>
<li><code>00</code> is the unused segment </li>
<li><code>01</code> for code</li>
<li><code>10</code> for the heap</li>
<li><code>11</code> for the stack</li>
</ul>
</li>
<li>Our virtual address looks like this:</li>
</ul>
<center><img src="./images/virtual_add.png"></center>
<ul>
<li>The base register for each segment contains the physical address of a linear page table for that segment</li>
<li>Each process has now 3 page tables</li>
<li>On context switch, this registers are changed</li>
</ul>
<p>Main differences with this hybrid approach and the first one: </p>
<ul>
<li>The segment page table will only have the entries that are allocated. Unallocated pages between the stack and the heap no long take up space in a page table </li>
</ul>
<p>Problems: </p>
<ul>
<li>If we have a large but sparsely used heap, for example, we can still end up with a lot of page table waste</li>
<li>This solutions causes external fragmentation, page tables now can be of arbitrary size, thus finding free space for them in memory is more complicated. </li>
</ul>
<h2 id="multi-level-page-tables"><a class="header" href="#multi-level-page-tables">Multi-Level page tables</a></h2>
<p><em>It turns the linear page table into something like a tree.</em>
The basic idea behind a multi-level page table is simple: </p>
<ol>
<li>Chop up the page table into page-sized units</li>
<li>If an entire page of page-table entries is invalid, don't allocate that page of the page table at all</li>
<li>To track whether a page of the page table is valid, use a new structure, called the page directory (note: Each process has his own page directory structure)</li>
<li>The page directory, can be used to tell you where a page of the page table is, or to tell if the entire page of the page table contains no valid pages</li>
</ol>
<center><img src="./images/multi-level-pt.png"></center>
<p>On this image we can see: </p>
<ul>
<li><strong>PDBR:</strong> Page directory base registers,  this holds the physical address where the page directory is.</li>
<li>The page directory: Maps to a page that contains a page table (this can be initialized or not initialized )</li>
<li>Just two page-table are mark as valid (1), hence only this 2 are initialized in memory</li>
</ul>
<p>The page directory: </p>
<ul>
<li>It contains one entry pert page of the page table</li>
<li>It consist of a number of page directory entries (PDE) which has a <em>valid</em> bit and a page frame number (to what frame number that page table maps to)</li>
<li>If the PDE is valid, it means that at least one element on the page table is valid </li>
</ul>
<h3 id="advantages-of-multi-level-pages"><a class="header" href="#advantages-of-multi-level-pages">Advantages of multi-level pages</a></h3>
<ul>
<li>Only allocates page-tables space in proportion to the amount of address space you are using</li>
<li>If carefully constructed, each portion of the page table first neatly within a page, making it easier to manage memory </li>
</ul>
<h3 id="cost-of-multi-level-pages"><a class="header" href="#cost-of-multi-level-pages">Cost of multi-level pages</a></h3>
<p>On TLB miss, two loads from memory will be required to get the right translation information from the page table, one for the page directory and one for the PTE itself. </p>
<h2 id="example-multi-level-page-tables"><a class="header" href="#example-multi-level-page-tables">Example multi-level page tables</a></h2>
<p>We have the following system: </p>
<ul>
<li>Small address space of size 16KB</li>
<li>64-byte pages</li>
<li>14-bit virtual address space</li>
<li>8 bits for the VPN</li>
<li>6 bits for the offset</li>
<li>A linear page table would have 2\({^8}\) entries</li>
</ul>
<p>For this example we have virtual page 0 and 1 for code, virtual page 4 and 5 for the heap, virtual pages 254 and 255 for the stack. The rest is <strong>unused</strong> </p>
<center><img src="./images/address_space_22.png"></center>
<h3 id="building-a-two-level-page-table"><a class="header" href="#building-a-two-level-page-table">Building a two-level page table</a></h3>
<ul>
<li>We start with our full linear page table, and break it up into page-sized units (each page is 64 bytes in size). </li>
<li>Full table has 256 entries</li>
<li>Assume each PTE is 4 bytes, hence our entire table is using 256 * 4 bytes = 1024 bytes</li>
<li>Because our page size is 64, we are using 16 pages for our page table</li>
</ul>
<p>Todo: How to take a VPN and use it to index first into the page directory and then into the page of the page table. </p>
<p>First, to index the page directory: We can do this by using 4 bits of our VPN bits, we need 4 bits because we have 16 possible pages to reference, 2\({^4}\) = 16.</p>
<center><img src="./images/vpn_page_dir.png"></center>
From here we can use the page directory index to get our page directory entry (PDE) with a simple calculation: `PDEAddr = PageDirBase + (PDIndex * sizeof(PDE)` 
- This is our page directory entry is marked invalid, we know that the access is invalid and raise and exception. 
- If the PDE is valid, we fetch the page table entry (PTE) from the page of the page table pointed to by this page directory entry, to find the PTE we use the remaining bits in our VPN
<p><img src="pte_index.png" alt="PTE index" /></p>
<p>This can be then used to index into the page table itself, here <code>PDE.PFN</code> is the page frame number of the page (or the address of the first element on the PDE) which is the indexed to the page table index we want: </p>
<pre><code>PTEAddr = (PDE.PFN &lt;&lt; SHIFT) + (PTIndex * sizeof(PTE))
</code></pre>
<h3 id="more-than-two-levels"><a class="header" href="#more-than-two-levels">More than two levels</a></h3>
<p>We have the following situation: </p>
<ul>
<li>30 bit virtual address space (the max address number I can represent is 30 bits)</li>
<li>A 512 bytes page (max number I can represent is 9 bits)</li>
<li>Hence our virtual space uses 21 bits to reference the page and 9 bits to reference the exact memory address inside that page </li>
</ul>
<p>Our goal when constructing a multi-level page table: Make each piece of the page table fit within a single page (meaning our page directory must fit inside a single page). </p>
<h4 id="how-to-determine-how-many-levels-are-needed-on-a-multi-level-table"><a class="header" href="#how-to-determine-how-many-levels-are-needed-on-a-multi-level-table">How to determine how many levels are needed on a multi-level table?</a></h4>
<ul>
<li>First we start by deterring how many page-table entries fit within a page: On our example, a page has a size of 512 bytes, and a PTE has a size of 4 bytes, which means a page can fit up to 128 PTEs. This means that we need 7 bits to represent the 128 values a page-table entry may have. </li>
</ul>
<center><img src="./images/pteentry.png"></center>
<ul>
<li>As we can see, we have 7 bits for the page table index (to index PTE) and 14 bits for the page directory entries </li>
<li>If we have 14 bits for the page directory index, that would mean 2\({^{14}}\) entries on our page directory, which spans not one page but 128, and thus our goal of making the directory entry fit on one page fails. 
To fix this, we create a further level of the tree, by spiting the page directory itself into multiple pages, and then adding another page directory on top of that. A memory reference number would look like this: </li>
</ul>
<center><img src="./images/final-mutlilevel.png"></center>
<center><i>Note: This "more than two levels" part is incomplete</i></center>
<div style="break-before: page; page-break-before: always;"></div><h1 id="14-beyond-physical-memory---mechanisms"><a class="header" href="#14-beyond-physical-memory---mechanisms">14. Beyond Physical Memory - Mechanisms</a></h1>
<p>We've assumed that every address space of every running process fits into memory, we will now relax this assumptions, and assume that we wish to support many concurrently running large address space. </p>
<p>To support large address spaces, the OS needs a place to stash away portions of address spaces that currently aren't in great demand, currently the most used place to stash this is the hard drive, we call this stash portion the swap space. </p>
<h2 id="swap-space"><a class="header" href="#swap-space">Swap space</a></h2>
<p>A space on the disk for moving pages back and fort, we swap pages out of memory to this space, and we swap pages into memory from this space. </p>
<h3 id="example-5"><a class="header" href="#example-5">Example</a></h3>
<p>We have a 3 page physical memory and an 8-page swap space. We also have 3 processes (Proc 0, Proc 1 and Proc 2) and they are actively sharing physical memory, there's also a 4th process that has all his pages on the swap space, hence is not running. </p>
<p><img src="swap_example1.png" alt="" /></p>
<h2 id="the-present-bit"><a class="header" href="#the-present-bit">The present bit</a></h2>
<p>On our page table entry, we need to ad a bit entry that represents if the desired page is either on swap space or in physical memory.</p>
<ul>
<li>If the present bit is set to one, it means the page is present in physical memory and everything goes as &quot;normal&quot;</li>
<li>If the present bit is set to zero, the page is not in memory, but rather on disk somewhere, this causes a page fault. On page fault, the OS needs to handle this exception via a page-fault handler. </li>
</ul>
<h2 id="the-page-fault"><a class="header" href="#the-page-fault">The page fault</a></h2>
<p>There are two type of systems on TLB misses: </p>
<ul>
<li>Hardware managed TLBs</li>
<li>Software managed TLBs
On both of them, there OS has a <strong>page-fault handler</strong>, this will swap the page into memory in order to service the page fault. </li>
</ul>
<h3 id="how-does-the-os-finds-the-pages-on-swap-space"><a class="header" href="#how-does-the-os-finds-the-pages-on-swap-space">How does the OS finds the pages on swap space</a></h3>
<ul>
<li>The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address</li>
<li>When the OS finds a page-fault, it looks for the in the PTE to find the address on disk and issues the request to disk to fetch the page into memory</li>
<li>When the I/O completes, the OS updates the page table to mark the page as present, it updates the PFN field on the PTE to the proper in memory location and retry the instruction</li>
</ul>
<h2 id="what-if-memory-is-full"><a class="header" href="#what-if-memory-is-full">What if memory is full?</a></h2>
<p>When the memory is full, the OS might choose to kick out some pages to swap space in order to free up some space on memory. The policy to do this is known as <strong>page-replacement policy</strong></p>
<h2 id="page-fault-control-flow"><a class="header" href="#page-fault-control-flow">Page fault control flow</a></h2>
<p>What the hardware does during translation</p>
<pre><code class="language-pseudocode">VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT
(Success, TlbEntry) = TLB_Lookup(VPN) 
if (Success == True) // TLB Hit (no need to translate by accessing memory) 
	if (CanAccess(TlbEntry.ProtectBits) == True) 
		Offset = VirtualAddress &amp; OFFSET_MASK
		PhysAddr = (TlbEntry.PFN &lt;&lt; SHIFT) | Offset
		Register = AccessMemory(PhysAddr)
	else
		RaiseException(PROTECTION_FAULT)
else  // TLB Miss
	PTEAddr = PTBR + (VPN * sizeof(PTE))
	PTE = AccessMemory(PTEAddr)
	if (PTE.Valid == False)
		RaiseException(SEGMENTATION_FAULT)
	else
		if (CanAccess(PTE.ProtectBits) == False)
			RaiseException(PROTECTION_FAULT)
		else if (PTE.Present == True)
			// assuming hardware-managed TLB
			TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
			RetryInstruction()
		else if (PTE.Present == False)
			RaiseException(PAGE_FAULT)
</code></pre>
<p>What the OS does upon a page fault</p>
<pre><code class="language-pseudocode">PFN = FIndFreePhysicalPage()
if (PFN == -1)              // No free page found
	PFN = EvictPage()
DiskRead(PTE.DiskAddr, PFN) // Sleep (waiting for I/O)
PTE.present = True          // Update page table with present bit and translation (PFN)
PTE.PFN = PFN
RetryInstruction()          // Retry instruction
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="15-physical-memory---policies"><a class="header" href="#15-physical-memory---policies">15. Physical Memory - Policies</a></h1>
<p>When there's no much memory free, the operating system is forced to start paging out pages to make room for actively-used pages. Deciding which page to evict and which page not to is described in the replacement policy. </p>
<h2 id="cache-management"><a class="header" href="#cache-management">Cache management</a></h2>
<ul>
<li>We can view memory as cache: <em>main memory holds some subset of all the pages in the system</em></li>
<li>Our goal: Picking a replacement policy for this cache to minimize the number of cache misses (minimize number of times we fetch pages from disk)</li>
<li>A nice metric to measure the cache misses and hits is the average memory access time (AMAT) for a program: 
$$
AMAT = T_{M} + (M_{Miss} \cdot T_D)
$$</li>
<li>\(T_M\) cost of accessing memory</li>
<li>\(T_{D}\) cost of accessing disk</li>
<li>\(P_{Miss}\) probability of a cache miss</li>
<li>We always pay the cost of accessing the data in memory</li>
</ul>
<h3 id="example-6"><a class="header" href="#example-6">Example</a></h3>
<p>We have: </p>
<ul>
<li>4KB address space</li>
<li>256-byte pages</li>
<li>Virtual address has two components, a 4-bits VPN (Used to reference the page frame number on a page table) and an 8 bits offset (Used to calculate the offset from a given page address)</li>
<li>Process can access a total of 16 virtual pages</li>
</ul>
<p>Now assume we have the following address <code>0x000</code>, <code>0x100</code>, <code>0x200</code>, <code>0x300</code>, <code>0x400</code>, <code>0x500</code>, <code>0x600</code>, <code>0x700</code>, <code>0x800</code> and <code>0x900</code> where each of these addresses reference the beginning of a page (the first element of a page)
All pages referenced on the previous address are on memory except for page number 3 (<code>0x300</code>). Hence when trying to access each of these addresses we will encounter the following behavior:</p>
<pre><code>hit, hit, hit, miss, hit, hit, hit, hit, hit, hit
</code></pre>
<p>Our hit rate would be 90%, miss rate 10%, hence \(P_{miss} = 0.1\). To calculate AMAT, we need to know the cost of accessing memory and the cost of accessing disk, let's assume that the cost of accessing to memory (\(T_M\)) is around 100 nano seconds and the cost to access disk (\(T_{D}\)) is around 10 miliseconds, we have the following AMAT :
$$
AMAT = T_{M} + (M_{Miss} \cdot T_{D}) = 100ns + 0.1 \cdot 10ms
$$
Which is
$$
= 1,0001 ms
$$</p>
<p>As you can see the cost of disk is so high compared to the cost of reading to memory that we clearly need to avoid as many misses as possible or we will run slowly at the rate of the disk. </p>
<h2 id="optimal-replacement-policy"><a class="header" href="#optimal-replacement-policy">Optimal Replacement Policy</a></h2>
<ul>
<li>The optimal  replacement policy is a simple policy that replace the page that will be accessed furthest in the future, resulting in the fewest possible cache misses. (Easy to say, really hard to implement)</li>
<li>If we need to throw a page, why not just throw the page that we will need the furthest from now</li>
</ul>
<h3 id="example-7"><a class="header" href="#example-7">Example</a></h3>
<p>We access the following virtual pages: 0,1,2,0,1,3,,3,1,2,1 </p>
<center><img src="./images/access_example.png"></center>
<p>As expected we start with 3 misses, we load the pages to memory, then when miss when accessing 3, here we look to the future to check which page we will be accessing last, we see that 0 is acceded intermediately after and that 1 is also accessed a little after that, then we see that the last one is 2, hence we evict 2. After this, we we access 2 (the last miss on the table) we see that both 3 and 0 are valid eviction candidates so we kick either one. </p>
<ul>
<li>Hit rate: 
[HitRate = \frac{Hits}{Hits + Misses} = \frac{6}{6+5} = 54.5% ]</li>
<li>Hit rate <em>modulo</em> (Ignore the first miss for a given page): 
[HitRateMod = \frac{Hits}{Hits + Misses} = \frac{6}{6+1} = 85.7% ]</li>
</ul>
<p>This policy will only be useful when comparing other alternatives to see how far are we from the &quot;perfect&quot; policy</p>
<h2 id="simple-policy-fifo"><a class="header" href="#simple-policy-fifo">Simple policy: FIFO</a></h2>
<ul>
<li>Pages are placed in a queue when they enter the system</li>
<li>When a replacement occurs: The page on the tail of the queue is evicted</li>
</ul>
<p>Let's examine the first example (on Optimal Replacement Policy): </p>
<center><img src="./images/FifoCacheExample.png"></center>
<p>We again begin with 3 misses, the we encounter 2 hits and a miss on access to page 3, since our first in was 0, then this page is evicted. We get a miss on 0 right after that, so we evict 1 because it was the first one to get into the list. Then again we get a miss with 1 and 2 and evict the corresponding pages. </p>
<ul>
<li>Hit rate: 
[HitRate = \frac{Hit} {Hit + Miss} = \frac{4}{4 + 7} = 36.4% ]</li>
<li>Hit rate <em>modulo</em>
[HitRateMod = \frac{Hit}{Hit + Miss} = \frac{4}{4 + 3} = 57.1% ]
We see a much worse performance, because FIFO can't determine the importance of blocks; even though 0 had been accessed a number of times, FIFO still kicks it out.</li>
</ul>
<h2 id="simply-policy-random"><a class="header" href="#simply-policy-random">Simply Policy: Random</a></h2>
<ul>
<li>It simply picks a random page to replace under memory pressure.</li>
<li>It is simple to implement</li>
<li>It isn't too intelligent in picking which blocks to evict</li>
</ul>
<p>On our example above, after running the random policy 1000 times we see the following results:</p>
<center><img src="./images/random_plot.png"></center>
<p>We see that about 40% of the time, Random is as good as optimal achieving 6 hits no the example trace. Sometimes it does much worse tho, achieving 2 or less hits. How random achieves is purely luck.</p>
<h2 id="using-history-lru"><a class="header" href="#using-history-lru">Using History: LRU</a></h2>
<p>We need a smarter policy, FIFO and Random might kick important pages that will be accessed right after they are evicted. </p>
<ul>
<li>We can use <em>history</em> as a guide, if a program has acceded a page in the near past, it is likely to access it again in the near future. </li>
<li>Frequency can also be used: If a page has been accessed many times, perhaps it should not be evicted</li>
<li>Recency of access: A The more recently a page has been accessed, perhaps the more like it will be accessed again 
All this policies are based on the principle of locality
<strong>Principle of locality:</strong> An observation about programs and their behavior, programs tend to access certain code sequences and data structures quite frequently, we should thus try to use history ot figure out which pages are important and keep those pages in memory when it comes to eviction time </li>
</ul>
<p><strong>LRU: Least-Frequently-Used</strong> policy replaces the least-frequently-used page when an eviction must take place. 
<strong>LRU: Least-Recently-Used</strong> policy replaces the least-recently-used page when an eviction must take place. </p>
<center><img src="./images/lru_history.png"></center>
<p>(It's like a FIFO but based on time)
When we encounter a miss on 3, we see which one was the least recently used, in this case is 2 so this is evicted, next we get a miss on 2, and we see that 0 is the least recently used so we evict that. </p>
<h2 id="workload-examples"><a class="header" href="#workload-examples">Workload examples</a></h2>
<h3 id="1-no-locality"><a class="header" href="#1-no-locality">1. No locality</a></h3>
<p>In our first example, our workload has no locality, meaning that each reference is to a random page, within the set of accessed pages.</p>
<ul>
<li>The workloads accesses 100 unique pages over time, choosing the next at random</li>
<li>Overall, 10.000 pages are accessed</li>
</ul>
<center><img src="./images/no_locality.png"></center>
<p>As we can see, LRU, FIFO and Random behave the same when dealing with workflow with no locality
Conclusions from this plot: </p>
<ol>
<li>When there's no locality, it doesn't matter  which realistic policy we choose</li>
<li>When the cache is large enough to fit the entire workload, it also doesn't matter which policy to use. </li>
<li>Optimal (OPT) performs noticeable better than the realistic policies because it can &quot;peek&quot; to the future</li>
</ol>
<h3 id="2-the-80-20-workload"><a class="header" href="#2-the-80-20-workload">2. The 80-20 workload</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="17-concurrency---an-introduction"><a class="header" href="#17-concurrency---an-introduction">17. Concurrency - An Introduction</a></h1>
<h2 id="threads-characteristics"><a class="header" href="#threads-characteristics">Threads characteristics</a></h2>
<ul>
<li>Thread process abstraction: Instead of a process having a single point of execution (single PC), a multi-threaded program has more than one point of execution.</li>
<li>Threads within the same process share the same address space. They can access the same data. </li>
<li>It has a program counter (PC), that track where the program is fetching instructions from. </li>
<li>Each thread has his own private registers used for computations. When we switch between T1 to T2, a context switch happens to store the registers of T1 to memory and restore the ones of T2 from memory. </li>
<li>To store the state of each thread in a process, we use one or more thread control blocks (TCBs)</li>
<li>The main difference between a context switch on threads to the one in process is that in threads the address space remains the same.</li>
<li>Each threads has his own stack, but they share the same heap. </li>
<li>Having many stacks, limits how much they can grow, this isn't a problem since stacks do not generally have to be very large. </li>
</ul>
<h2 id="why-use-threads"><a class="header" href="#why-use-threads">Why use threads</a></h2>
<p>There are two major reasons: </p>
<ol>
<li>Parallelism: On a multi CPU environment, using a thread per CPU to portions of a task can reduce the time it takes to finish the task. </li>
<li>Avoid blocking on I/O: While one thread waits for an I/O operation to finish, another one can use the CPU to make other work. Hence you avoid blocking your process when doing I/O tasks. 
Why not use multiple processes instead of threads? Because threads share the same address space and thus it's easier to share data. </li>
</ol>
<h2 id="example-thread-creation"><a class="header" href="#example-thread-creation">Example thread creation</a></h2>
<pre><code class="language-C">// main.c
#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

void *mythread(void *arg) {
  printf(&quot;%s\n&quot;, (char *)arg);
  return NULL;
}

int main(int argc, char *argv[]) {
  pthread_t p1, p2;
  int rc;
  printf(&quot;main: begin\n&quot;);
  pthread_create(&amp;p1, NULL, mythread, &quot;A&quot;);
  pthread_create(&amp;p2, NULL, mythread, &quot;B&quot;);

  // join waits for the threads to finish
  pthread_join(p1, NULL);
  pthread_join(p2, NULL);
  printf(&quot;main: end\n&quot;);
  return 0;
}

// To compile run: gcc -pthread main.c -o main
</code></pre>
<p>In this program, we can't know which thread will get to run first. A might be printed before B, or B before A. It's up to the thread scheduler which thread to run first. </p>
<h2 id="shared-data"><a class="header" href="#shared-data">Shared data</a></h2>
<p>We have the following C program: </p>
<pre><code class="language-C">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

static volatile int counter = 0;

void *mythread(void *arg) {
  printf(&quot;%s: begin\n&quot;, (char *)arg);
  int i;
  for (i = 0; i &lt; 1e7; i++) {
    counter = counter + 1;
  }
  printf(&quot;%s: done\n&quot;, (char *)arg);
  return NULL;
}

int main(int argc, char *argv[]) {
  pthread_t p1, p2;
  int rc;
  printf(&quot;main: begin\n&quot;);
  pthread_create(&amp;p1, NULL, mythread, &quot;A&quot;);
  pthread_create(&amp;p2, NULL, mythread, &quot;B&quot;);

  // join waits for the threads to finish
  pthread_join(p2, NULL);
  pthread_join(p1, NULL);
  printf(&quot;main: done with both (counter = %d)\n&quot;, counter);
  return 0;
}
// To compile run: gcc -pthread main.c
</code></pre>
<p>Here we have two thread wishing to update the same global variable <code>counter</code>. Each worker (thread created) is trying to add a number to the shared variable <code>counter</code> 10 million times (1e7) in a loop. Thus, since we have two thread and the initial value is 0, we expect the result to be 20.000.000. </p>
<p>We compile and run our program: </p>
<pre><code class="language-bash">$ gcc -pthread main.c -o main
$ ./main
main: begin
A: begin
B: begin
A: done
B: done
main: done with both (counter = 10250346)
</code></pre>
<p>We clearly didn't got the desired result. Let's try again: </p>
<pre><code class="language-bash">$ ./main
main: begin
A: begin
B: begin
B: done
A: done
main: done with both (counter = 11403750)
</code></pre>
<p>Each time we run the program, not only we got the wrong result, but we got a different result. </p>
<h2 id="uncontrolled-scheduling"><a class="header" href="#uncontrolled-scheduling">Uncontrolled Scheduling</a></h2>
<p>To understand these weird error, we need to understand the code sequence that the compiler generates for the update to <code>counter</code>. In assembly the instructions look something like this: </p>
<pre><code class="language-S">mov    0x8049a1c,%eax
add    $0x1,%eax
mov    %eax,0x8049a1c
</code></pre>
<p><em>Note: To see the assembly code of your program, you can run <code>objdump -d main </code></em></p>
<p>Here the variable <code>counter</code> is located at address <code>0x8049a1c</code>. 
In this 3 instruction assembly we have that:</p>
<ol>
<li>The <code>mov</code> instruction, moves the value at address <code>0x8049a1c</code> (our counter value) to register <code>eax</code></li>
<li>The <code>add</code> instruction, adds 1 to the content of the register <code>eax</code></li>
<li>The content of <code>eax</code> is stored back into memory at the same address</li>
</ol>
<p>Let's image the one of our 2 threads (Thread 1) enters this region of code. It load the value of counter (let's say the value is currently at 50) to the register <code>eax</code>, then it adds 1 to the register; thus <code>eax = 51</code>. Now Thread 1 is interrupted, the OS saves the state of T1 and now is T2 turn to run. Thread 2 loads the value of the counter (50 still since Thread 1 didn't write into memory) into <code>eax</code>, adds 1 (thus <code>eax = 51</code>) and saves that back to memory. Now Thread 1 runs again, and when restoring the registers, we have that <code>eax</code> is 51, now thread 1 saves <code>eax</code> into memory and <code>counter = 51</code> (again).</p>
<p>What happened? The code to increment <code>counter</code> has been run twice, but <code>counter</code>, which started at 50, is now only equal to 51. A correct version of this programs should have result int the variable <code>counter</code> equal to 52.</p>
<p>This is called a race condition, where the result depends on the timing of execution. When multiple thread executing a piece of code can result in a race condition, we call this piece of code a critical section. </p>
<h2 id="the-wish-for-atomicity"><a class="header" href="#the-wish-for-atomicity">The wish for Atomicity</a></h2>
<p>One way of solving this, is having a single instruction that in a single step, does whatever we needed to do and thus removing the possibility of an interrupt on the middle of the task. Something like this:</p>
<pre><code class="language-S">memory-add 0x8049a1c, $0x1
</code></pre>
<p>Atomically, in this context means as a unit, or &quot;all or none&quot;. What we want to do, is execute the previous 3 instruction atomically:</p>
<pre><code class="language-S">mov    0x8049a1c,%eax
add    $0x1,%eax
mov    %eax,0x8049a1c
</code></pre>
<p>Generally we don't have a single instruction to avoid data races, hence we need to implement <strong>synchronization primitives</strong></p>
<h2 id="crux-how-to-support-synchronization"><a class="header" href="#crux-how-to-support-synchronization">Crux: How to Support Synchronization</a></h2>
<h2 id="one-more-problem-waiting-for-another"><a class="header" href="#one-more-problem-waiting-for-another">One more problem: Waiting For Another</a></h2>
<p>This problem happens when a thread must wait for another to complete some action before it continues. For example when a process performs an I/O operation and is put to sleep; when the I/O completes, the process need to be roused from its slumber so it can continue. What mechanisms we need to support this type of sleeping/waking interaction that is common in multi-threaded programs? </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="thread-api"><a class="header" href="#thread-api">Thread API</a></h1>
<p>Here we present the API the OS provides for thread creation and control</p>
<h2 id="thread-creation"><a class="header" href="#thread-creation">Thread creation</a></h2>
<pre><code class="language-C">#include &lt;pthread.h&gt;

int pthread_create(pthread_t *thread, 
	const pthread_attr_t *attr, 
	void *(*start_routine)(void *), 
	void *arg
);
</code></pre>
<p>Arguments of this function: </p>
<ol>
<li><code>thread</code>: A pointer to a structure of type <code>pthread_t</code>, this struct is used to interact with threads</li>
<li><code>aatr</code>: Used to specify any attributes this thread might have (Stack size, scheduling priority, etc).</li>
<li><code>start_routine</code>: Function the thread should start running. This is a pointer to a function with return type <code>void *</code>. </li>
<li><code>arg</code> is the argument to be passed to the function where the thread begins execution. </li>
</ol>
<h2 id="thread-completion"><a class="header" href="#thread-completion">Thread completion</a></h2>
<p>What if we want to wait for a thread to complete? You must call the routine <code>pthread_join()</code></p>
<pre><code class="language-C">pthread_join(pthread_t th, void **thread_return);
</code></pre>
<p>This routine takes two arguments: </p>
<ol>
<li><code>th</code>: Of type <code>pthread_t</code> indicates which thread to wait for. </li>
<li><code>thread_return</code>: It's a pointer to a <code>void *</code> pointer, used to store the return value of the function that the thread is executing. If we don't care about the return value, we can use <code>NULL</code></li>
</ol>
<p>You should never return a pointer to a value allocated in the stack of a thread. When the thread ends its execution, it is destroyed alongside his stack, hence the value that the pointer in pointing to is lost and we get UB.</p>
<h2 id="locks"><a class="header" href="#locks">Locks</a></h2>
<p>Mutual exclusion to a critical section via locks. The most basic pair of routines to use for this purpose is provided by the following: </p>
<pre><code class="language-C">pthread_mutex_lock(pthread_mutex_t *mutex);
pthread_mutex_unlock(pthread_mutex_t *mutex);
</code></pre>
<p>These can be used to create a &quot;container&quot; around our critical block of code: </p>
<pre><code class="language-C">pthread_mutex_t lock;
...
pthread_mutex_lock(&amp;lock);
x = x + 1; // Or whatever your critical section is
pthread_mutex_unlock(&amp;lock);
</code></pre>
<p>This works as follows:</p>
<ul>
<li>If no other thread holds the lock when <code>pthread_mutex_lock(&amp;lock);</code> is called, then the thread will acquire the lock. </li>
<li>If other thread holds the lock, <code>pthread_mutex_lock(&amp;lock);</code> will &quot;wait&quot; for the lock to become available. </li>
<li>The thread makes the critical section operation. </li>
<li>The thread releases the lock using <code>pthread_mutex_unlock(&amp;lock);</code></li>
</ul>
<p>This code snippet is badly written (for simplicity) since: </p>
<ul>
<li>The lock is poorly initialized, and it should be initialized either using <code>PTHREAD_MUTEX_INITIALIZER</code> at compile time or <code>pthread_mutex_init()</code> at runtime. </li>
<li>The code doesn't check error codes when calling lock and unlock.</li>
</ul>
<h2 id="condition-variables"><a class="header" href="#condition-variables">Condition Variables</a></h2>
<p>Use when a thread has to wait for other thread on some condition state. The two primary routines look like this: </p>
<pre><code class="language-C">pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
pthread_cond_signal(pthread_cond_t *cond);
</code></pre>
<p>To use a condition variable, we need a lock that is associated with this condition. When calling either of the above routines, the lock should be held.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="19-locks"><a class="header" href="#19-locks">19. Locks</a></h1>
<p>We want to execute a series of instruction atomically, due to the presence of interrupts, we couldn't. To solve this we introduce lock, which we put around critical section and ensure this sections is executed as a single atomic instruction. </p>
<h2 id="the-basic-idea"><a class="header" href="#the-basic-idea">The basic idea</a></h2>
<p>We have a critical section that looks like this:</p>
<pre><code class="language-C">balance = balance + 1;
</code></pre>
<p>To use a lock, we add some code around the critical section like this:</p>
<pre><code class="language-C">lock_t mutex; // Some globally-allocated lock `mutex`
...
lock(&amp;mutex);
balance = balance + 1;
unlock(&amp;mutex);
</code></pre>
<ul>
<li>Lock is just a variable </li>
<li>It stores the state of the lock at any given time</li>
<li>It's either available or acquired</li>
<li>We can store more data in the lock such as the current thread that's holding it, or a list of some kind for ordering lock acquisition</li>
</ul>
<p>The general flow of locks are describe here: </p>
<ul>
<li>Calling <code>lock()</code> tries to acquire the lock</li>
<li>If no other thread holds the lock, the thread will acquire the lock and enter the critical section</li>
<li>If another thread tries to acquire the lock, the thread will not return while the lock is held by another thread. This way, other threads are prevented from entering the critical section. .</li>
<li>Once the owner of the lock calls <code>unlock()</code>, the lock is now available again to other threads. </li>
<li>If there are no other threads waiting for the lock, the state of the lock is set to free. </li>
<li>If there are waiting threads, one of them will notice and will acquire the lock and enter critical section.</li>
</ul>
<p>With locks we make sure than only a single thread can access a critical section of code. </p>
<h2 id="pthread-locks"><a class="header" href="#pthread-locks">Pthread locks</a></h2>
<p>The name the POSIX library uses for a lock is <code>mutex</code> as it is used to provide mutual exclusion between threads. When you see the following threads code, assume that's doing the same thing as above: </p>
<pre><code class="language-C">pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

Pthread_mutex_lock(&amp;lock); // wrapper; exits on failure
balance = balance + 1;
Pthread_mutex_unlock(&amp;lock);
</code></pre>
<p>Passing a variable to lock and unlock helps us avoid locking all threads with one lock (coarse-grained locking strategy) and doing a more specific thread lock (a more fine-grained approach) </p>
<h2 id="evaluating-locks"><a class="header" href="#evaluating-locks">Evaluating Locks</a></h2>
<p>To build locks we must defined some evaluation criteria:</p>
<ul>
<li>Does the lock provides mutual exclusion. Does the lock work, preventing multiple threads from accessing a critical section.</li>
<li>Fairness. Does each thread contending the lock gets a fair shot at acquiring it once its free. </li>
<li>Performance. The time overheads added by using the lock. 
<ul>
<li>When a single thread grabs and releases the lock, what's the overhead of doing so</li>
<li>Is there a performance overhead when multiple threads are contending for a lock. </li>
<li>How does the lock performs when there are multiple CPU's involved and threads on each contending lock. </li>
</ul>
</li>
</ul>
<h2 id="controlling-interrupts"><a class="header" href="#controlling-interrupts">Controlling interrupts</a></h2>
<p>One solution to implement mutex was to disable interrupts for critical sections. The could would look like this: </p>
<pre><code class="language-C">void lock() {
	DisableInterrupts();
} 
void unlock() {
	EnableInterrupts();
}
</code></pre>
<p>Assuming that we are running on a single processor system, by turning off interrupts before entering a critical section, we ensure that the code inside this section won't be interrupted </p>
<h4 id="benefits"><a class="header" href="#benefits">Benefits</a></h4>
<ul>
<li>Simplicity: Easy to implement and to grasp. </li>
</ul>
<h4 id="negatives"><a class="header" href="#negatives">Negatives</a></h4>
<ul>
<li>This allows any calling thread to perform a privileged operation, we must trust that this is not abused. </li>
<li>Greedy program calls lock and never <code>unlock</code> hence taking over the entire CPU</li>
<li>Buggy or malicious program could call <code>lock</code> and enter a loop, OS never regains control and we must restart the entire system</li>
<li>Does not work on multiple CPUs systems, if a thread disables interrupts, a thread running on a different CPU can still access critical section. </li>
<li>Running off interrupts for extended period of time can lead to interrupts becoming lost. </li>
<li>Inefficient: Code that mask and unmask interrupts are CPU inefficient. </li>
</ul>
<p>This negatives <strong>might</strong> be acceptable when running OS level programs, since the OS trust it self. </p>
<h2 id="just-using-loadsstores"><a class="header" href="#just-using-loadsstores">Just Using Loads/Stores</a></h2>
<p>Block using single flag variable (<code>flag</code>) to indicate whether some thread has possession of a lock. </p>
<ul>
<li>The first thread that enters the critical section calls <code>lock()</code> which check if the <code>flag</code> is set to 1 (in this case, is not), and then sets the flag to 1 to indicate that the thread now holds the lock. </li>
<li>When the thread finishes with the critical section, it calls <code>unlock()</code> which clears the flag</li>
<li>If another thread calls <code>lock()</code> while the lock is held, it will find that the flag is set to 1 so it will simply <em>spin-wait</em> in a while loop for that thread to call <code>unlock</code> and clear the flag</li>
<li>Once that first thread does clear the flag, the waiting thread fall out of the while loop, sets the flag to 1 for itself and proceeds into the critical section .</li>
</ul>
<pre><code class="language-C">typedef struct __lock_t { int flag; } lock_t;

void init(lock_t *mutex) {
	// 0 -&gt; lock is available, 1 -&gt; held
	mutex-&gt;flag = 0;
}
void lock(lock_t *mutex) {
	while (mutex-&gt;flag == 1) // TEST the flag
		;
	mutex-&gt;flag = 1; // now SET it!
}
void unlock(lock_t *mutex) {
	mutex-&gt;flag = 0;
}
</code></pre>
<p>This implementation has two errors.</p>
<ol>
<li>The first one is that we can easily produce a case where both threads set the flag to 1, and both threads are thus able to enter critical section. </li>
<li>The second being a performance error, the <code>lock</code> routine does a <strong>spin-waiting</strong>, which wastes time waiting for another thread to release a lock but at the same time running on the CPU. On a single CPU system, how can a thread be waiting for a lock while using the CPU if the other threads needs to use it to free the lock? Doesn't makes sense. </li>
</ol>
<h2 id="building-working-spin-locks-with-test-and-set"><a class="header" href="#building-working-spin-locks-with-test-and-set">Building working spin locks with test and set</a></h2>
<p>System designers started to invent hardware support for locking. The test-and-set instruction is one implementation of this hardware support. 
In this example we see test-and-set in practice via C code: </p>
<pre><code class="language-C">int TestAndSet(int *old_ptr, int new) {
	int old = *old_ptr; // Fetch old value at old ptr;
	*old_ptr = new; // Store 'new' into old_ptr
	return old; // return the old value
}

typedef struct __lock_t {
	int flag;
} lock_t;

void init(lock_t *lock) {
	// 0: lock is available, 1: lock is held
	lock-&gt;flag = 0;
}

void lock(lock_t *lock) {
	while (TestAndSet(&amp;lock-&gt;flag, 1) == 1)
		; // spin-wait (do nothing)
}

void unlock(lock_t *lock) {
	lock-&gt;flag = 0;
}
</code></pre>
<p>First case, a thread calls <code>lock</code>:</p>
<ul>
<li>No other threads currently holds the lock (thus <code>flag</code> is 0).</li>
<li>Thread calls <code>TestAndSet(flag, 1)</code>, which returns the old value (0)</li>
<li>The thread breaks the loop since the returned value is 0</li>
<li>The thread also atomically set the value of <code>flag</code> to 1 thus indicating that the thread is now held </li>
<li>We the threads finishes execution of the critical section, <code>unlock</code> is called. 
The second case is:</li>
<li>Other thread already has the lock held (<code>flag</code> is 1)</li>
<li>Another thread calls <code>TestAndSet(flag, 1)</code></li>
<li><code>TestAndSet</code> returns the old values which is 1, while simultaneously setting it to 1 again </li>
<li>As long as other threads holds the lock, 1 will be returned and thus this thread will spin and spin until the lock is finally released. </li>
</ul>
<p>This <code>TestAndSet</code> is actually an atomtic instruction implemented on the hardware level, hence it can't be interrupted. Hence we ensure that only one thread acquire the lock. </p>
<h2 id="evaluating-spin-locks"><a class="header" href="#evaluating-spin-locks">Evaluating Spin Locks</a></h2>
<p>Given the previous spin lock we can evaluate it. </p>
<ul>
<li>Correctness: Does it provide mutual exclusion? Yes, it only allows a single thread to enter critical section at a time. </li>
<li>Fairness. Spin locks doesn't provide any fairness guarantees. A thread might spin forever, under contention and wont execute the critical section. </li>
<li>Performance: We analyze this on a single process and a multi processor system:
<ul>
<li>Single processor: Performance is bad. If a thread holding the lock is preempted within the critical section, the scheduler might run every other thread, each of them runs for a slice of time. A waste of CPU cycles. </li>
<li>On multiple CPU's. Performance is reasonably well. Thread A hold a lock in CPU 1, thread B spin in CPU2, spinning  to wait the lock on another processor doesn't waste many cycles in this case. </li>
</ul>
</li>
</ul>
<h2 id="compare-and-swap"><a class="header" href="#compare-and-swap">Compare-And-Swap</a></h2>
<p>Hardware primitive that some systems provide. The C pseudocode looks like this: </p>
<pre><code class="language-C">int CompareAndSwap(int *ptr, int expected, int new) {
	int original = *ptr;
	if (original == expected)
		*ptr = new;
	return original;
}
</code></pre>
<p>And the lock instruction: </p>
<pre><code class="language-C">void lock(lock_t *lock) {
	while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)
		; // spin
}
</code></pre>
<p>This tests if the value that <code>ptr</code> is pointing to, is the <code>expected</code>. If so, update the value that <code>ptr</code> is pointing to, to <code>new</code>. Finally return the <code>original</code> value. </p>
<h2 id="load-linked-and-store-conditional"><a class="header" href="#load-linked-and-store-conditional">Load-Linked and Store-Conditional</a></h2>
<p>Hardware instruction, the C pseudocode looks like this: </p>
<pre><code class="language-C">int LoadLinked(int *ptr) {
	return *ptr;
}
int StoreConditional(int *ptr, int value) {
	if (no update to *ptr since LoadLinked to this address) {
		*ptr = value;
		return 1; // success!
	} else {
		return 0;
	}
}

void lock(lock_t *lock) {
	while (1) {
		while (LoadLinked(&amp;lock-&gt;flag) == 1)
			; // Spin until it's zero
		if (StoreConditional(&amp;lock-&gt;flag, 1) == 1)
			return; // If set-it-to-1 was a success: all done
					// Otherwise: try all over again
	}
}

void unlock(lock_t *lock) {
	lock-&gt;flag = 0;
}
</code></pre>
<ul>
<li>Store-conditional: It only succeeds if no intervening store to the address has taken place. </li>
<li><code>lock()</code>: A thread spins waiting for the flag to be set to 0</li>
<li>Once so, thread tries to acquire the lock via the store-conditional</li>
</ul>
<h2 id="fetch-and-add"><a class="header" href="#fetch-and-add">Fetch-and-Add</a></h2>
<p>Hardware primitive, atomically increments a value while returning the old value at a particular address. C pseudocode: </p>
<pre><code class="language-c">int FetchAndAdd(int *ptr) {
	int old = *ptr;
	*ptr = old + 1;
	return old;
}
typedef struct __lock_t {
	int ticket;
	int turn;
} lock_t;

void lock_init(lock_t *lock) {
	lock-&gt;ticket = 0;
	lock-&gt;turn = 0;
}

void lock(lock_t *lock) {
	int myturn = FetchAndAdd(&amp;lock-&gt;ticket);
	while (lock-&gt;turn != myturn)
		; // spin 
}

void unlock(lock_t *lock) {
	lock-&gt;turn = lock-&gt;turn + 1;
}
</code></pre>
<ul>
<li>Ticket and turn value. </li>
<li>When a thread wishes to acquire a lock it does an atomic fetch-and-add on the ticket value, the value is considered the thread's turn (<code>myturn</code>). </li>
<li>The globally shared <code>lock-&gt;turn</code> is used to determine which thread's turn it is.</li>
<li>When <code>myturn == turn</code> then the thread can run</li>
<li>Unlock is done by adding 1 to the <code>lock-&gt;turn</code> value.</li>
</ul>
<h2 id="spin-performance"><a class="header" href="#spin-performance">Spin performance</a></h2>
<p>We have 2 threads: </p>
<ul>
<li>Thread 0 is in critical section and thus has a lock held, then it get interrumpted. </li>
<li>Thread 1 tries to acquire the lock, but finds it held, it start looping and wasting cpu time. </li>
<li>Hardware support can't fix this problem, OS support is needed. </li>
</ul>
<h2 id="spin-solution-yield"><a class="header" href="#spin-solution-yield">Spin solution: Yield</a></h2>
<p>When you are going to spin, instead just give up the CPU to another thread. This can be represented in C code: </p>
<pre><code class="language-C">void init() {
	flag = 0;
}
void lock() {
	while (TestAndSet(&amp;flag, 1) == 1)
		yield(); // give up the cpu 
}
void unlock() {
	flag = 0;
}
</code></pre>
<p>Yield is a system call that moves the caller from the running state to the ready state. Thus promoting promotes another thread to running.</p>
<ul>
<li>Good enough we have few threads</li>
<li>Bad when we start having a lot of threads, and we have to yield a lot of times, hence still wasting CPU time (still better that the no yield approach)</li>
<li>The cost of context switch is present in this solution. </li>
<li>Starvation it's still present in this solution</li>
</ul>
<h2 id="using-queues"><a class="header" href="#using-queues">Using Queues</a></h2>
<p>A thread has either to spin waiting for the lock or yield the CPU, either way there's waste and no prevention of starvation. 
This can be improved with OS support in terms of two calls: </p>
<ul>
<li><code>park()</code> to put calling thread to sleep</li>
<li><code>unpark(threadID)</code> to wake a particular thread as designated by <code>threadID</code></li>
<li>This can be used to put the caller to sleep with it tries to acquire a held lock and wakes it when the lock is free 
C code representation of this: </li>
</ul>
<pre><code class="language-C">typedef struct __lock_t {
  int flag; // If flag is set to 0 we can run the thread, else we add it to tue queue and park it
  int guard; // This is to control access of flag and queue
  queue_t *queue;
} lock_t;

void lock_init(lock_t *m) {
  m-&gt;flag = 0;
  m-&gt;guard = 0;
  queue_init(m-&gt;queue);
}

void lock(lock_t *m) {
  while (TestAndSet(&amp;m-&gt;guard, 1) == 1) // We try to acquire read and write access to flag and queue
    ;  // acquire guard lock
  if (m-&gt;flag == 0) { // If flag = 0, we can run (held lock)
    m-&gt;flag = 1;
    m-&gt;guard = 0;
  } else { // else we add it to tue queue, park and continue
			// hold held by other thread
    queue_agdd(m-&gt;queue, gettid());
	setpark(); 
    m-&gt;guard = 0;
    park();
  }
}

void unlock(lock_t *m) {
  while (TestAndSet(&amp;m-&gt;guard, 1) == 1)
    ;  // acquire guard lock
  if (queue_empty(m-&gt;queue)) {
    m-&gt;flag = 0;
  } else {
    unpark(queue_remove(m-&gt;queue));
  }
  m-&gt;guard = 0;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="20-lock-based-concurrent-data-structures"><a class="header" href="#20-lock-based-concurrent-data-structures">20. Lock-based Concurrent Data Structures</a></h1>
<p>Adding locks to data structures to make them usable by threads makes the structure thread safe. </p>
<h2 id="crux-how-to-add-locks-to-data-structures"><a class="header" href="#crux-how-to-add-locks-to-data-structures">Crux: How to add locks to data structures</a></h2>
<p>How should we add locks to data structures, in order to make it work correctly, high performance, many threads at once? i.e. concurrently. </p>
<h2 id="concurrent-counters"><a class="header" href="#concurrent-counters">Concurrent Counters</a></h2>
<p>We have the following counter: </p>
<pre><code class="language-C">typedef struct __counter_t {
	int value;
	pthread_mutex_t lock;
} counter_t;

void init(counter_t *c) {
	c-&gt;value = 0;
	Pthread_mutex_init(&amp;c-&gt;lock, NULL);
}

void increment(counter_t *c) {
	Pthread_mutex_lock(&amp;c-&gt;lock);
	c-&gt;value++;
	Pthread_mutex_unlock(&amp;c-&gt;lock);
}

void decrement(counter_t *c) {
	Pthread_mutex_lock(&amp;c-&gt;lock);
	c-&gt;value--;
	Pthread_mutex_unlock(&amp;c-&gt;lock);
}

int get(counter_t *c) {
	Pthread_mutex_lock(&amp;c-&gt;lock);
	int rc = c-&gt;value;
	Pthread_mutex_unlock(&amp;c-&gt;lock);
	return rc;
}
</code></pre>
<ul>
<li>This structures has a single lock, which is acquired when we write and the released when returning from  the write call. </li>
<li>This code has performance costs:
<ul>
<li>Benchmarking this code, shows us that from a 0.03 seconds that it takes to run on a single threads, it jumps to nearly 5 seconds on 2 threads.</li>
</ul>
</li>
</ul>
<h3 id="scalable-counting"><a class="header" href="#scalable-counting">Scalable counting</a></h3>
<p>Definition: When see the threads complete just as quickly on multiple processors as the single threads does on one.</p>
<p>The most famous technique to attack this problem is called <strong>approximate counter</strong>:</p>
<ul>
<li>Multiple threads on multiple CPU's </li>
<li>Each threads has a local counter, and there a single global counter. </li>
<li>When a thread running on a given core wishes to incremente the counter, it increments its local counter</li>
<li>Access to this local pointer is sync via a lock</li>
<li>To keep the global counter up to date, local values are periodically transferred to the global counter. </li>
<li>How often this local-to-global transfer occurs is given by a threshold <em>S</em>:
<ul>
<li>The small <em>S</em> is, the more the counter behaves like the non-scalable counter above</li>
<li>The bigger <em>S</em>, the more scalable but less precise </li>
<li><em>S</em> is local to the CPU, meaning that for each CPU there's a timer that when it reaches <em>S</em>, it transfers his counter to the Global counter. </li>
</ul>
</li>
</ul>
<h2 id="concurrent-linked-list"><a class="header" href="#concurrent-linked-list">Concurrent Linked List</a></h2>
<p>Check the code on the book, page 362. </p>
<ul>
<li>The code acquires a lock on insertion and releases it once finished. </li>
<li>The code acquires a lock for lookup, since we don't want our list to change when we are searching for an element inside of it. </li>
</ul>
<h3 id="scaling-linked-lists"><a class="header" href="#scaling-linked-lists">Scaling Linked Lists</a></h3>
<ul>
<li>Instead of having a single lock for the entire list, we have a lock per node. </li>
<li>This is called hand-over-hand locking</li>
<li>When traversing the list, the code first grabs the next node's lock and then releases the current node's lock</li>
<li>This has performance issues since grabbing a lock for each node can be a time expensive task.</li>
</ul>
<h2 id="concurrent-queues"><a class="header" href="#concurrent-queues">Concurrent queues</a></h2>
<p>Check the code on the book, page 365. </p>
<ul>
<li>We have two locks, one for the tail and one for the head</li>
<li>Thanks to this locks, we can perform concurrent enqueue and dequeue operations. </li>
<li>We can also add a dummy code to separate the head part of the queue, with the tail part. </li>
</ul>
<h2 id="concurrent-hash-tables"><a class="header" href="#concurrent-hash-tables">Concurrent hash tables</a></h2>
<p>Code at page 366</p>
<ul>
<li>Instead of having a lock for the entire structures we have a lock per hash bucket. </li>
<li>This enables many concurrent operations to take place. </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="21-condition-variables"><a class="header" href="#21-condition-variables">21. Condition variables</a></h1>
<ul>
<li>Threads might want to check a condition is true before continuing its execution. </li>
<li>A parent thread might wish to check whether a child thread has completed before continuing </li>
<li>We would want to put the parent to sleep until the condition is met. </li>
</ul>
<h2 id="definition-and-routines"><a class="header" href="#definition-and-routines">Definition and Routines</a></h2>
<ul>
<li><strong>Condition variable</strong>: An explicit queue that threads can put themselves on when some state of execution (i.e. some condition) is not as desired (by waiting on the condition)</li>
<li>When some other threads changes said state, it can then wake one (or more) of those waiting threads and thus allow them to continue. </li>
<li>To use condition variables we have <code>wait()</code> and <code>signal()</code>:
<ul>
<li><code>wait()</code> is used by threads to put themselves to sleep. </li>
<li><code>signal()</code> is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting this condition</li>
</ul>
</li>
</ul>
<pre><code class="language-C">int done = 0;
pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
pthread_cond_t c = PTHREAD_COND_INITIALIZER;

void thr_exit() {
	Pthread_mutex_lock(&amp;m);
	done = 1;
	Pthread_cond_signal(&amp;c);
	Pthread_mutex_unlock(&amp;m);
}

void *child(void *arg) {
	printf(&quot;child\n&quot;);
	thr_exit();
	return NULL;
}

void thr_join() {
	Pthread_mutex_lock(&amp;m);
	while (done == 0) 
		Pthread_cond_wait(&amp;c, &amp;m);
	Pthread_mutex_unlock(&amp;m);
}

int main(int argc, char *argv[]) {
	printf(&quot;parent: begin\n&quot;);
	pthread_t p;
	Pthread_create(&amp;p, NULL, child, NULL);
	thr_join();
	printf(&quot;parent: end\n&quot;);
	return 0;
}
</code></pre>
<p><strong>In this code we have two cases to consider, the first:</strong></p>
<ul>
<li>Parent create child thread</li>
<li>Parent continues execution and jumps to <code>thr_join</code></li>
<li>It acquires the lock and check if <code>done</code> is 1</li>
<li>Since <code>done</code> is not 1 yet, it calls <code>Pthread_cond_wait</code> which takes the condition and a lock</li>
<li><code>Pthread_cond_wait</code> releases the lock and puts the thread to sleep</li>
<li>The child thread runs and calls <code>thr_exit</code></li>
<li><code>thr_exit</code> acquires the lock and sets done to 1 and sends a signal to threads that are watching for <code>c</code></li>
<li>The parent threads wakes up with the lock held and checks if <code>done</code> is 1</li>
<li><code>done</code> is 1 so we jump to <code>Pthread_mutex_unlock</code> to release the lock </li>
</ul>
<p><strong>On the seconds options we have:</strong></p>
<ul>
<li>Parent creates thread</li>
<li>Child runs executes the code and set <code>done</code> to 1 </li>
<li>The parent calls <code>thr_join</code> and since <code>done</code> is 1, it skips the <code>Pthread_cond_wait</code> part and directly calls <code>Pthread_mutex_unlock</code> and continues execution. </li>
</ul>
<h2 id="the-producerconsumer-bounded-buffer-problem"><a class="header" href="#the-producerconsumer-bounded-buffer-problem">The Producer/Consumer (Bounded Buffer) Problem</a></h2>
<ul>
<li>We have one or more producer threads and one or more consumer threads. </li>
<li>Producers generate data and place them in a buffer</li>
<li>Consumers grab the items from the buffer and consume them in some way</li>
<li>The bounded buffer is a shared resource, we rquire sync access to it. 
Example: </li>
</ul>
<pre><code class="language-C">void *producer(void *arg) {
	int i; 
	int loops = (int) arg;
	for (i = 0; i)
}
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
